This file is a merged representation of the entire codebase, combining all repository files into a single document.

Additional Info:
----------------

We had this error when chossing the hybrid model 
[ComfyUI-HotReloadHack] Reloaded module ComfyUI-IF_Zonos
got prompt
Loading Transformer model...
Generating:  31%|â–ˆâ–ˆâ–ˆ       | 802/2588 [00:20<00:44, 39.78it/s]
Prompt executed in 30.42 seconds
got prompt
Warning: Could not process prefix audio: The size of tensor a (64) must match the size of tensor b (4691) at non-singleton dimension 1
Generating:  31%|â–ˆâ–ˆâ–ˆâ–      | 814/2588 [00:20<00:44, 39.95it/s]
Prompt executed in 20.87 seconds
[ComfyUI-HotReloadHack] Reloaded module ComfyUI-IF_Zonos
[ComfyUI-HotReloadHack] Reloaded module ComfyUI-IF_Zonos
Exception in callback _ProactorBasePipeTransport._call_connection_lost(None)
handle: <Handle _ProactorBasePipeTransport._call_connection_lost(None)>
Traceback (most recent call last):
  File "C:\Python312\Lib\asyncio\events.py", line 88, in _run
    self._context.run(self._callback, *self._args)
  File "C:\Python312\Lib\asyncio\proactor_events.py", line 165, in _call_connection_lost
    self._sock.shutdown(socket.SHUT_RDWR)
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host
[ComfyUI-HotReloadHack] Reloaded module ComfyUI-IF_Zonos
[ComfyUI-HotReloadHack] Reloaded module ComfyUI-IF_Zonos
got prompt
Loading Transformer model...
Generating:  31%|â–ˆâ–ˆâ–ˆ       | 800/2588 [00:20<00:45, 39.17it/s]
Prompt executed in 30.82 seconds
got prompt
Loading Hybrid model...
Generating:   0%|          | 0/2588 [00:00<?, ?it/s]Error in generate_speech: query and key must have the same dtype
!!! Exception during processing !!! query and key must have the same dtype
Traceback (most recent call last):
  File "C:\Users\SOYYO\AppData\Local\Programs\@comfyorgcomfyui-electron\resources\ComfyUI\execution.py", line 327, in execute
    output_data, output_ui, has_subgraph = get_output_data(obj, input_data_all, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)
                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\SOYYO\AppData\Local\Programs\@comfyorgcomfyui-electron\resources\ComfyUI\execution.py", line 202, in get_output_data
    return_values = _map_node_over_list(obj, input_data_all, obj.FUNCTION, allow_interrupt=True, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\SOYYO\AppData\Local\Programs\@comfyorgcomfyui-electron\resources\ComfyUI\execution.py", line 174, in _map_node_over_list
    process_inputs(input_dict, i)
  File "C:\Users\SOYYO\AppData\Local\Programs\@comfyorgcomfyui-electron\resources\ComfyUI\execution.py", line 163, in process_inputs
    results.append(getattr(obj, func)(**inputs))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\SOYYO\Documents\ComfyUI\custom_nodes/ComfyUI-IF_Zonos\IF_Zonos.py", line 200, in generate_speech
    raise e
  File "C:\Users\SOYYO\Documents\ComfyUI\custom_nodes/ComfyUI-IF_Zonos\IF_Zonos.py", line 160, in generate_speech
    codes = self.model.generate(
            ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\SOYYO\Documents\ComfyUI\.venv\Lib\site-packages\torch\utils\_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\SOYYO\Documents\ComfyUI\custom_nodes/ComfyUI-IF_Zonos\zonos\model.py", line 279, in generate
    logits = decode_one_token(input_ids, inference_params, cfg_scale, allow_cudagraphs=cg)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\SOYYO\Documents\ComfyUI\custom_nodes/ComfyUI-IF_Zonos\zonos\model.py", line 157, in _decode_one_token
    logits = self._compute_logits(hidden_states, inference_params, cfg_scale)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\SOYYO\Documents\ComfyUI\custom_nodes/ComfyUI-IF_Zonos\zonos\model.py", line 110, in _compute_logits
    last_hidden_states = self.backbone(hidden_states, inference_params)[:, -1, :].unsqueeze(1)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\SOYYO\Documents\ComfyUI\.venv\Lib\site-packages\torch\nn\modules\module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\SOYYO\Documents\ComfyUI\.venv\Lib\site-packages\torch\nn\modules\module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\SOYYO\Documents\ComfyUI\custom_nodes/ComfyUI-IF_Zonos\zonos\backbone\_mamba_ssm.py", line 47, in forward
    hidden_states, residual = layer(hidden_states, residual, inference_params)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\SOYYO\Documents\ComfyUI\.venv\Lib\site-packages\torch\nn\modules\module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\SOYYO\Documents\ComfyUI\.venv\Lib\site-packages\torch\nn\modules\module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\SOYYO\Documents\ComfyUI\.venv\Lib\site-packages\mamba_ssm\modules\block.py", line 67, in forward
    hidden_states = self.mixer(hidden_states, inference_params=inference_params, **mixer_kwargs)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\SOYYO\Documents\ComfyUI\.venv\Lib\site-packages\torch\nn\modules\module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\SOYYO\Documents\ComfyUI\.venv\Lib\site-packages\torch\nn\modules\module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\SOYYO\Documents\ComfyUI\.venv\Lib\site-packages\mamba_ssm\modules\mha.py", line 289, in forward
    context = self._apply_rotary_update_kvcache_attention(q, kv, inference_params)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\SOYYO\Documents\ComfyUI\.venv\Lib\site-packages\mamba_ssm\modules\mha.py", line 152, in _apply_rotary_update_kvcache_attention
    context = flash_attn_with_kvcache(
              ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\SOYYO\Documents\ComfyUI\.venv\Lib\site-packages\flash_attn\flash_attn_interface.py", line 1589, in flash_attn_with_kvcache
    out, softmax_lse = flash_attn_gpu.fwd_kvcache(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: query and key must have the same dtype

Prompt executed in 87.77 seconds
Generating:   0%|          | 0/2588 [00:01<?, ?it/s]



besides that there is an issue with the queality aof the audio generation vaying the speed etc.. 

we need to improve IF_Zonos.py base on the code from Zonos.py and zonos/gradio_interface.py


================================================================
Repository Structure
================================================================
__init__.py
IF_Zonos.py
setup.py
speak_test.py
step1-espeak.py
Zonos.py
zonos/autoencoder.py
zonos/backbone/__init__.py
zonos/backbone/_mamba_ssm.py
zonos/backbone/_torch.py
zonos/codebook_pattern.py
zonos/conditioning.py
zonos/config.py
zonos/gradio_interface.py
zonos/model.py
zonos/sample.py
zonos/sampling.py
zonos/speaker_cloning.py
zonos/utils.py

================================================================
Repository Files
================================================================

================
File: __init__.py
================
# __init__.py
import os
from .IF_Zonos import NODE_CLASS_MAPPINGS, NODE_DISPLAY_NAME_MAPPINGS

# Specify the web directory for any web components
WEB_DIRECTORY = os.path.join(os.path.dirname(os.path.realpath(__file__)), "web")

__all__ = ["NODE_CLASS_MAPPINGS", "NODE_DISPLAY_NAME_MAPPINGS", "WEB_DIRECTORY"]

================
File: IF_Zonos.py
================
# IF_Zonos.py
import os
import torch
import torchaudio
import folder_paths
from .zonos.model import Zonos
from .zonos.conditioning import make_cond_dict, supported_language_codes

class IF_ZonosTTS:
    """
    ComfyUI node for Zonos text-to-speech generation with emotion control.
    """
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model = None
        self.model_type = None
        # Disable torch inductor to avoid C++ compilation issues
        torch._dynamo.config.suppress_errors = True

    @classmethod
    def INPUT_TYPES(s):
        # Get path to silence file relative to the custom node's directory
        silence_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), "assets", "silence_100ms.wav")
               
        return {
            "required": {
                "model_type": (["Transformer", "Hybrid"], {"default": "Transformer"}),
                "text": ("STRING", {"multiline": True, "default": "Welcome to Zonos text to speech!"}),
                "language": (supported_language_codes, {"default": "en-us"}),
                "happiness": ("FLOAT", {"default": 0.6, "min": 0.0, "max": 1.0, "step": 0.05}),
                "sadness": ("FLOAT", {"default": 0.05, "min": 0.0, "max": 1.0, "step": 0.05}),
                "disgust": ("FLOAT", {"default": 0.05, "min": 0.0, "max": 1.0, "step": 0.05}),
                "fear": ("FLOAT", {"default": 0.05, "min": 0.0, "max": 1.0, "step": 0.05}),
                "surprise": ("FLOAT", {"default": 0.05, "min": 0.0, "max": 1.0, "step": 0.05}),
                "anger": ("FLOAT", {"default": 0.05, "min": 0.0, "max": 1.0, "step": 0.05}),
                "other": ("FLOAT", {"default": 0.5, "min": 0.0, "max": 1.0, "step": 0.05}),
                "neutral": ("FLOAT", {"default": 0.6, "min": 0.0, "max": 1.0, "step": 0.05}),
                "cfg_scale": ("FLOAT", {"default": 2.0, "min": 1.0, "max": 5.0, "step": 0.1}),
                "min_p": ("FLOAT", {"default": 0.1, "min": 0.0, "max": 1.0, "step": 0.01}),
                "seed": ("INT", {"default": 420, "min": 0, "max": 0xfffffffff}),
            },
            "optional": {
                "speaker_audio": ("AUDIO",),
                "prefix_audio": ("AUDIO", {
                    "default": silence_path if os.path.exists(silence_path) else None
                }),
            }
        }

    RETURN_TYPES = ("AUDIO",)
    FUNCTION = "generate_speech"
    CATEGORY = "ImpactFramesðŸ’¥ðŸŽžï¸/Audio"

    def load_model_if_needed(self, model_type):
        if self.model is None or self.model_type != model_type:
            if self.model is not None:
                del self.model
                torch.cuda.empty_cache()
            
            print(f"Loading {model_type} model...")
            if model_type == "Transformer":
                self.model = Zonos.from_pretrained("Zyphra/Zonos-v0.1-transformer", device=self.device)
            else:
                self.model = Zonos.from_pretrained("Zyphra/Zonos-v0.1-hybrid", device=self.device)
            
            self.model.to(self.device)
            self.model.bfloat16()
            self.model.eval()
            self.model_type = model_type

    def preprocess_audio(self, waveform, sample_rate, target_sr):
        """Safely process audio input."""
        try:
            waveform = waveform.squeeze()  # Remove any extra dimensions
            
            # Handle multi-channel audio
            if waveform.dim() > 1:
                waveform = waveform.mean(dim=0)  # Convert to mono
            
            # Add batch dimension if needed
            if waveform.dim() == 1:
                waveform = waveform.unsqueeze(0)
            
            # Resample if needed
            if sample_rate != target_sr:
                waveform = torchaudio.functional.resample(waveform, sample_rate, target_sr)
                
            return waveform.to(self.device)
        except Exception as e:
            print(f"Error preprocessing audio: {e}")
            return None

    def generate_speech(self, model_type, text, language, happiness, sadness, 
                    disgust, fear, surprise, anger, other, neutral,
                    cfg_scale, min_p, seed, speaker_audio=None, prefix_audio=None):
        try:
            self.load_model_if_needed(model_type)
            
            # Set random seed  
            torch.manual_seed(seed)
            
            # Process speaker audio if provided
            speaker_embedding = None
            if speaker_audio is not None:
                waveform = self.preprocess_audio(
                    speaker_audio["waveform"],
                    speaker_audio["sample_rate"], 
                    16000
                )
                if waveform is not None:
                    speaker_embedding = self.model.make_speaker_embedding(
                        waveform,
                        16000
                    )
                    speaker_embedding = speaker_embedding.to(self.device, dtype=torch.bfloat16)

            # Process prefix audio if provided
            audio_prefix_codes = None 
            if prefix_audio is not None:
                # Use the same approach as in the gradio interface:
                waveform = prefix_audio["waveform"]
                # Ensure mono channel with channel dimension preserved
                if waveform.dim() > 1:
                    waveform = waveform.mean(dim=0, keepdim=True)
                else:
                    waveform = waveform.unsqueeze(0)
                sr_prefix = prefix_audio["sample_rate"]
                try:
                    # Use autoencoder's preprocess for proper handling
                    waveform = self.model.autoencoder.preprocess(waveform, sr_prefix)
                    waveform = waveform.to(self.device, dtype=torch.float32)
                    # Autoencoder.encode expects a batch dimension; add one if missing.
                    if waveform.dim() == 2:
                        waveform = waveform.unsqueeze(0)
                    audio_prefix_codes = self.model.autoencoder.encode(waveform)
                except Exception as e:
                    print(f"Warning: Could not process prefix audio: {e}")
                    audio_prefix_codes = None

            # Create emotion tensor
            emotion_tensor = torch.tensor(
                [[happiness, sadness, disgust, fear, surprise, anger, other, neutral]],
                device=self.device
            )
            
            # Create condition dictionary
            cond_dict = make_cond_dict(
                text=text,
                language=language,
                speaker=speaker_embedding,
                emotion=emotion_tensor,
                device=self.device
            )

            # Generate audio (using autocast on CUDA for improved performance/quality)
            with torch.inference_mode():
                if self.device == "cuda":
                    with torch.amp.autocast(device_type="cuda"):
                        conditioning = self.model.prepare_conditioning(cond_dict)
                        codes = self.model.generate(
                            prefix_conditioning=conditioning,
                            audio_prefix_codes=audio_prefix_codes,
                            max_new_tokens=86 * 30,
                            cfg_scale=cfg_scale,
                            batch_size=1,
                            sampling_params=dict(min_p=min_p),
                            disable_torch_compile=True
                        )
                        # Decode audio after generation
                        wav_out = self.model.autoencoder.decode(codes)
                else:
                    conditioning = self.model.prepare_conditioning(cond_dict)
                    codes = self.model.generate(
                        prefix_conditioning=conditioning,
                        audio_prefix_codes=audio_prefix_codes,
                        max_new_tokens=86 * 30,
                        cfg_scale=cfg_scale,
                        batch_size=1,
                        sampling_params=dict(min_p=min_p),
                        disable_torch_compile=True
                    )
                    wav_out = self.model.autoencoder.decode(codes)
                    
                # Ensure proper shape for the output audio
                wav_out = wav_out.cpu()
                if wav_out.dim() == 1:
                    wav_out = wav_out.unsqueeze(0)  # Add channels dimension
                elif wav_out.dim() == 2:
                    if wav_out.size(0) > 1:
                        wav_out = wav_out[0:1, :]  # Take only the first channel to preserve cadence
                elif wav_out.dim() == 3:
                    wav_out = wav_out.squeeze(0)  # Remove batch dimension if present
                    
                # Return audio in ComfyUI format
                return ({"waveform": wav_out.unsqueeze(0), 
                        "sample_rate": self.model.autoencoder.sampling_rate},)

        except Exception as e:
            print(f"Error in generate_speech: {str(e)}")
            raise e

    def time_shift(self, audio, speed):
        import torch.nn.functional as F
        rate = audio['sample_rate']
        waveform = audio['waveform']
        
        # Correct speed factor:
        # If speed > 1, audio should be slower (longer duration),
        # so we multiply the original length by speed.
        old_length = waveform.shape[-1]
        new_length = int(old_length * speed)
        
        # Resample audio using linear interpolation
        new_waveform = F.interpolate(
            waveform.unsqueeze(1),
            size=new_length,
            mode='linear',
            align_corners=False
        ).squeeze(1)
    
        return {"waveform": new_waveform, "sample_rate": rate}

NODE_CLASS_MAPPINGS = {
    "IF_ZonosTTS": IF_ZonosTTS  
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "IF_ZonosTTS": "Zonos Text to Speech ðŸŽ¤"
}

================
File: setup.py
================
import os
import subprocess
import sys
import time
import shutil
import ctypes
from pathlib import Path

class Colors:
    HEADER = '\033[95m'
    BLUE = '\033[94m'
    GREEN = '\033[92m'
    WARNING = '\033[93m'
    FAIL = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'

def print_status(message, status_type="info"):
    timestamp = time.strftime("%H:%M:%S")
    if status_type == "info":
        print(f"{Colors.BLUE}[{timestamp}] INFO: {message}{Colors.ENDC}")
    elif status_type == "success":
        print(f"{Colors.GREEN}[{timestamp}] SUCCESS: {message}{Colors.ENDC}")
    elif status_type == "warning":
        print(f"{Colors.WARNING}[{timestamp}] WARNING: {message}{Colors.ENDC}")
    elif status_type == "error":
        print(f"{Colors.FAIL}[{timestamp}] ERROR: {message}{Colors.ENDC}")
    elif status_type == "header":
        print(f"\n{Colors.HEADER}{Colors.BOLD}[{timestamp}] {message}{Colors.ENDC}\n")

def verify_espeak_library(dll_path):
    try:
        lib = ctypes.CDLL(dll_path)
        if hasattr(lib, 'espeak_Initialize'):
            result = lib.espeak_Initialize(0, 0, None, 0)
            if result >= 0:
                print_status(f"Successfully initialized eSpeak library at {dll_path}", "success")
                return True
        print_status("Library found but initialization failed", "error")
        return False
    except Exception as e:
        print_status(f"Failed to load eSpeak library: {e}", "error")
        return False

def prepare_environment():
    print_status("Preparing environment...", "header")
    
    if 'VIRTUAL_ENV' not in os.environ:
        print_status("No virtual environment detected!", "error")
        return False

    venv_path = os.environ['VIRTUAL_ENV']
    scripts_dir = os.path.join(venv_path, 'Scripts')
    site_packages = os.path.join(venv_path, 'Lib', 'site-packages')
    espeak_install = r"C:\Program Files\eSpeak NG"

    try:
        print_status("Installing phonemizer...")
        subprocess.run(['uv', 'pip', 'install', 'phonemizer==3.2.1'], check=True)
        
        phonemizer_dir = os.path.join(site_packages, 'phonemizer')
        bin_dir = os.path.join(phonemizer_dir, 'bin')
        os.makedirs(bin_dir, exist_ok=True)

        print_status("Copying eSpeak files...")
        files_to_copy = [
            ('espeak-ng.exe', 'executable'),
            ('libespeak-ng.dll', 'library'),
            ('libwinpthread-1.dll', 'dependency')
        ]
        
        for file_name, file_type in files_to_copy:
            src = os.path.join(espeak_install, file_name)
            if os.path.exists(src):
                for dest_dir in [scripts_dir, bin_dir]:
                    dest = os.path.join(dest_dir, file_name)
                    shutil.copy2(src, dest)
                    print_status(f"Copied {file_name} to {dest_dir}", "success")
            else:
                print_status(f"Warning: {file_name} not found in eSpeak installation", "warning")

        data_src = os.path.join(espeak_install, 'espeak-ng-data')
        if os.path.exists(data_src):
            for dest_dir in [scripts_dir, bin_dir]:
                data_dest = os.path.join(dest_dir, 'espeak-ng-data')
                if os.path.exists(data_dest):
                    shutil.rmtree(data_dest)
                shutil.copytree(data_src, data_dest)
                print_status(f"Copied espeak-ng-data to {dest_dir}", "success")
        
        dll_paths = [
            os.path.join(scripts_dir, 'libespeak-ng.dll'),
            os.path.join(bin_dir, 'libespeak-ng.dll')
        ]
        
        valid_dll = None
        for dll_path in dll_paths:
            if verify_espeak_library(dll_path):
                valid_dll = dll_path
                break
        
        if not valid_dll:
            print_status("Failed to verify eSpeak library in any location", "error")
            return False
        
        os.environ['PHONEMIZER_ESPEAK_LIBRARY'] = valid_dll
        os.environ['PHONEMIZER_ESPEAK_PATH'] = os.path.dirname(valid_dll)
        os.environ['PATH'] = f"{os.path.dirname(valid_dll)}{os.pathsep}{os.environ['PATH']}"
        
        print_status("Testing phonemizer...")
        test_script = """
import sys
if sys.platform == 'win32': sys.stdout.reconfigure(encoding='utf-8')
import phonemizer
from phonemizer.backend import EspeakBackend
backend = EspeakBackend('en-us', language_switch='remove-flags')
print("Phonemizer test successful!")
"""
        result = subprocess.run([sys.executable, '-c', test_script],
                              capture_output=True, text=True)
        
        if result.returncode == 0:
            print_status("Phonemizer test successful!", "success")
            return True
        else:
            print_status(f"Phonemizer test failed: {result.stderr}", "error")
            return False

    except Exception as e:
        print_status(f"Error during environment preparation: {e}", "error")
        return False

def install_packages():
    print_status("Starting package installation...", "header")
    
    try:
        print_status("Installing dependencies from requirements.txt and wheels...")
        # First install jaraco.functools which is a dependency
        subprocess.run(['uv', 'pip', 'install', 'jaraco.functools'], check=True)
        
        # Then install the rest of the requirements
        subprocess.run(['uv', 'pip', 'install', '--find-links=wheels/', '-r', 'requirements.txt'], check=True)
        
        print_status("Package installation completed", "success")

    except Exception as e:
        print_status(f"Error during package installation: {e}", "error")
        raise

def main():
    if sys.platform != 'win32':
        print_status("This script is for Windows only", "error")
        sys.exit(1)

    try:
        if not prepare_environment():
            print_status("Failed to prepare environment", "error")
            sys.exit(1)
        
        install_packages()
        print_status("Installation completed successfully", "success")
    except Exception as e:
        print_status(f"Installation failed: {str(e)}", "error")
        sys.exit(1)

if __name__ == "__main__":
    main()

================
File: speak_test.py
================
import espeakng

mySpeaker = espeakng.Speaker()
mySpeaker.say("Hello, world!")

================
File: step1-espeak.py
================
import os
import subprocess
import winreg
import sys
import time
import shutil

class Colors:
    HEADER = '\033[95m'
    BLUE = '\033[94m'
    GREEN = '\033[92m'
    WARNING = '\033[93m'
    FAIL = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'

def print_status(message, status_type="info"):
    timestamp = time.strftime("%H:%M:%S")
    if status_type == "info":
        print(f"{Colors.BLUE}[{timestamp}] INFO: {message}{Colors.ENDC}")
    elif status_type == "success":
        print(f"{Colors.GREEN}[{timestamp}] SUCCESS: {message}{Colors.ENDC}")
    elif status_type == "warning":
        print(f"{Colors.WARNING}[{timestamp}] WARNING: {message}{Colors.ENDC}")
    elif status_type == "error":
        print(f"{Colors.FAIL}[{timestamp}] ERROR: {message}{Colors.ENDC}")
    elif status_type == "header":
        print(f"\n{Colors.HEADER}{Colors.BOLD}[{timestamp}] {message}{Colors.ENDC}\n")

def get_espeak_ng_installation_path():
    """Find the espeak binary in the installation directory."""
    print_status("Searching for eSpeak NG installation...", "header")
    
    # First check common installation paths
    common_paths = [
        r"C:\Program Files\eSpeak NG",
        r"C:\Program Files (x86)\eSpeak NG",
    ]
    for path in common_paths:
        print_status(f"Checking path: {path}")
        dll_path = os.path.join(path, 'libespeak-ng.dll')
        if os.path.isfile(dll_path):
            print_status(f"Found eSpeak NG at: {path}", "success")
            return path
    
    print_status("eSpeak NG not found in common locations", "warning")
    return None

def install_espeak_ng():
    print_status("Starting eSpeak NG installation...", "header")
    try:
        print_status("Using winget to install eSpeak NG...")
        result = subprocess.run(
            ["winget", "install", "--id=eSpeak-NG.eSpeak-NG", "-e", "--silent",
             "--accept-source-agreements", "--accept-package-agreements"],
            capture_output=True,
            text=True,
            creationflags=subprocess.CREATE_NO_WINDOW
        )
        
        if result.returncode == 0:
            print_status("eSpeak NG installation completed successfully", "success")
        else:
            print_status(f"Installation output: {result.stdout}\nErrors: {result.stderr}", "warning")
            raise subprocess.CalledProcessError(result.returncode, result.args)
            
    except subprocess.CalledProcessError as e:
        print_status(f"Installation failed: {str(e)}", "error")
        raise RuntimeError("eSpeak NG installation failed")

def set_system_path(espeak_path):
    try:
        # Get the current system PATH
        key = winreg.OpenKey(winreg.HKEY_LOCAL_MACHINE, r'SYSTEM\CurrentControlSet\Control\Session Manager\Environment', 0, winreg.KEY_ALL_ACCESS)
        current_path = winreg.QueryValueEx(key, 'Path')[0]
        
        # Check if eSpeak path is already in PATH
        if espeak_path.lower() in current_path.lower():
            print_status("eSpeak NG already in system PATH", "success")
            return True
            
        # Add eSpeak path to PATH
        new_path = current_path + ';' + espeak_path
        winreg.SetValueEx(key, 'Path', 0, winreg.REG_EXPAND_SZ, new_path)
        winreg.CloseKey(key)
        print_status("Added eSpeak NG to system PATH", "success")
        
        # Notify about system update
        print_status("System PATH updated. You may need to restart your terminal or computer for changes to take effect.", "warning")
        return True
        
    except Exception as e:
        print_status(f"Failed to update system PATH: {str(e)}", "error")
        print_status("Please add the following path to your system PATH manually:", "warning")
        print_status(espeak_path, "info")
        return False

def setup_taproot_compatibility(espeak_dir):
    """Set up environment for taproot compatibility."""
    try:
        # Get virtual environment Scripts directory
        if 'VIRTUAL_ENV' not in os.environ:
            print_status("No virtual environment detected!", "error")
            return False

        scripts_dir = os.path.join(os.environ['VIRTUAL_ENV'], 'Scripts')
        bin_dir = os.path.join(os.environ['VIRTUAL_ENV'], 'bin')
        
        # Create bin directory if it doesn't exist
        os.makedirs(bin_dir, exist_ok=True)

        # Copy all necessary files to Scripts directory
        print_status("Copying eSpeak files to virtual environment...")
        
        # List of files to copy
        files_to_copy = [
            'espeak-ng.exe',
            'libespeak-ng.dll',
            'libwinpthread-1.dll'  # Common dependency
        ]
        
        for file_name in files_to_copy:
            src = os.path.join(espeak_dir, file_name)
            if os.path.exists(src):
                # Copy to Scripts
                dst_scripts = os.path.join(scripts_dir, file_name)
                shutil.copy2(src, dst_scripts)
                print_status(f"Copied {file_name} to Scripts directory", "success")
                
                # Copy to bin
                dst_bin = os.path.join(bin_dir, file_name)
                shutil.copy2(src, dst_bin)
                print_status(f"Copied {file_name} to bin directory", "success")

        # Copy espeak-ng-data directory
        data_src = os.path.join(espeak_dir, 'espeak-ng-data')
        if os.path.exists(data_src):
            data_dst_scripts = os.path.join(scripts_dir, 'espeak-ng-data')
            data_dst_bin = os.path.join(bin_dir, 'espeak-ng-data')
            
            if os.path.exists(data_dst_scripts):
                shutil.rmtree(data_dst_scripts)
            if os.path.exists(data_dst_bin):
                shutil.rmtree(data_dst_bin)
                
            shutil.copytree(data_src, data_dst_scripts)
            shutil.copytree(data_src, data_dst_bin)
            print_status("Copied espeak-ng-data directory", "success")

        # Create Unix-style symlink for taproot
        dll_path = os.path.join(scripts_dir, 'libespeak-ng.dll')
        unix_style_path = os.path.join(scripts_dir, 'libespeak.so.1')
        
        # Remove existing file if it exists
        if os.path.exists(unix_style_path):
            os.remove(unix_style_path)
        
        # Copy DLL to Unix-style name
        shutil.copy2(dll_path, unix_style_path)
        print_status("Created Unix-style library link for taproot", "success")

        # Set environment variables
        os.environ['PHONEMIZER_ESPEAK_LIBRARY'] = dll_path
        os.environ['PHONEMIZER_ESPEAK_PATH'] = scripts_dir
        os.environ['TAPROOT_ESPEAK_LIBRARY'] = unix_style_path
        
        print_status("Environment variables set:", "success")
        print_status(f"PHONEMIZER_ESPEAK_LIBRARY={dll_path}")
        print_status(f"PHONEMIZER_ESPEAK_PATH={scripts_dir}")
        print_status(f"TAPROOT_ESPEAK_LIBRARY={unix_style_path}")

        return True
    except Exception as e:
        print_status(f"Error setting up taproot compatibility: {e}", "error")
        return False

def verify_installation():
    """Verify eSpeak installation and environment setup."""
    try:
        result = subprocess.run(['espeak-ng', '--version'], 
                              capture_output=True, 
                              text=True)
        if result.returncode == 0:
            print_status("eSpeak NG installation verified", "success")
            print_status(f"Version: {result.stdout.strip()}")
            return True
        else:
            print_status("Failed to verify eSpeak NG installation", "error")
            return False
    except Exception as e:
        print_status(f"Error verifying installation: {e}", "error")
        return False

def main():
    if not sys.platform == 'win32':
        print_status("This script is for Windows only", "error")
        sys.exit(1)

    try:
        print_status("Starting Step 1: eSpeak NG Installation", "header")
        
        # Check if eSpeak is already installed
        espeak_path = get_espeak_ng_installation_path()
        if not espeak_path:
            print_status("eSpeak NG not found, starting installation...")
            install_espeak_ng()
            # Check again after installation
            espeak_path = get_espeak_ng_installation_path()
            if not espeak_path:
                raise RuntimeError("Failed to find eSpeak NG after installation")
        
        # Set system PATH
        path_success = set_system_path(espeak_path)
        
        # Set up taproot compatibility
        taproot_success = setup_taproot_compatibility(espeak_path)
        
        if path_success and taproot_success:
            # Verify installation
            if verify_installation():
                print_status("\nStep 1 completed successfully!", "success")
                print_status("Please follow these steps:", "info")
                print_status("1. Close this terminal", "info")
                print_status("2. Open a new terminal", "info")
                print_status("3. Run step2-install-packages.py", "info")
            else:
                print_status("Installation verification failed", "error")
        else:
            print_status("Step 1 completed with warnings. Please check the messages above.", "warning")
            
    except Exception as e:
        print_status(f"Installation failed: {str(e)}", "error")
        sys.exit(1)

if __name__ == "__main__":
    main()

================
File: Zonos.py
================
# Zonos.py
# Resource: https://github.com/sdbds/Zonos-for-windows/blob/main/gradio_interface.py

import os.path
from .Install import Install

import sys
import torch
import torchaudio
import hashlib
import folder_paths
import tempfile
import io
import re
import shutil
import subprocess

import comfy.model_management as mm
from comfy.utils import load_torch_file, ProgressBar, common_upscale

import torch._dynamo.config
import torch._inductor.config

zonos_path  = os.path.join(Install.zonosPath)
sys.path.insert(0, zonos_path)

Install.check_install()

from zonos.model import Zonos
from zonos.conditioning import make_cond_dict, supported_language_codes
from zonos.backbone import BACKBONES

device = mm.get_torch_device()

ZONOS_MODEL_PATH = os.path.join(folder_paths.models_dir, "zonos")

# Ensure directories exist
os.makedirs(ZONOS_MODEL_PATH, exist_ok=True)

def check_espeak_installation():
    """Check espeak installation and return the path"""
    try:
        # First check if espeak is in PATH
        espeak_path = shutil.which('espeak')
        if espeak_path:
            return espeak_path
            
        # Check common Windows installation paths
        common_paths = [
            r"C:\Program Files\eSpeak NG\espeak-ng.exe",
            r"C:\Program Files (x86)\eSpeak NG\espeak-ng.exe",
        ]
        
        for path in common_paths:
            if os.path.exists(path):
                # Verify it works by running a simple test
                try:
                    subprocess.run([path, "--version"], capture_output=True, text=True)
                    return path
                except:
                    continue
                    
        return None
    except Exception:
        return None

class ZonosGenerate:
    voice_reg = re.compile(r"\{([^\}]+)\}")
    model_types = ["Zyphra/Zonos-v0.1-transformer", "Zyphra/Zonos-v0.1-hybrid"]
    tooltip_seed = "Seed. -1 = random"
    tooltip_speed = "Speed. >1.0 slower. <1.0 faster"
    
    # Move class variables to top level for clarity
    CURRENT_MODEL_TYPE = None
    CURRENT_MODEL = None
    CURRENT_SPEAKER_HASH = None
    CURRENT_SPEAKER_EMBEDDING = None
    
    @staticmethod
    def hash_audio(waveform, sample_rate):
        """Compute hash of audio content for caching"""
        m = hashlib.sha256()
        m.update(waveform.cpu().numpy().tobytes())
        m.update(str(sample_rate).encode())
        return m.digest().hex()

    @staticmethod
    def get_model_path(model_name):
        """Get paths to model and config files, downloading if needed"""
        if model_name not in ZonosGenerate.model_types:
            raise ValueError(f"Invalid model type: {model_name}")
            
        # Create model-specific directory
        model_dir = os.path.join(ZONOS_MODEL_PATH, model_name.split('/')[-1])
        os.makedirs(model_dir, exist_ok=True)
        
        model_file = "model.safetensors"
        config_file = "config.json"
        model_path = os.path.join(model_dir, model_file)
        config_path = os.path.join(model_dir, config_file)
        
        if not os.path.exists(model_path) or not os.path.exists(config_path):
            print(f"Downloading Zonos model {model_name} to: {model_dir}")
            from huggingface_hub import snapshot_download
            
            # Download model files
            snapshot_download(
                repo_id=model_name,
                allow_patterns=["*.safetensors", "*.json"],
                local_dir=model_dir,
                local_dir_use_symlinks=False
            )
            
            # Download speaker embedding model if not exists
            speaker_model = "Zyphra/Zonos-v0.1-speaker-embedding"
            speaker_path = os.path.join(ZONOS_MODEL_PATH, "speaker_embedding.safetensors")
            speaker_config = os.path.join(ZONOS_MODEL_PATH, "config.json")
            
            if not os.path.exists(speaker_path) or not os.path.exists(speaker_config):
                print(f"Downloading speaker embedding model to: {ZONOS_MODEL_PATH}")
                snapshot_download(
                    repo_id=speaker_model,
                    allow_patterns=["*.safetensors", "*.json"],
                    local_dir=ZONOS_MODEL_PATH,
                    local_dir_use_symlinks=False
                )
            
        return model_path, config_path

    def is_voice_name(self, word):
        return self.voice_reg.match(word.strip())

    def split_text(self, speech):
        reg1 = r"(?=\{[^\}]+\})"
        return re.split(reg1, speech)

    def generate_audio(self, voices, chunks, seed, language="en-us", cfg_scale=2.0, min_p=0.15, audio_prefix_codes=None):
        if seed >= 0:
            torch.manual_seed(seed)
        else:
            torch.random.seed()

        generated_audio_segments = []
        
        for text in chunks:
            match = self.is_voice_name(text)
            if match:
                voice = match[1]
            else:
                voice = "main"
            if voice not in voices:
                print(f"Voice {voice} not found, using main.")
                voice = "main"
                
            text = ZonosGenerate.voice_reg.sub("", text)
            gen_text = text.strip()
            if gen_text == "":
                continue

            voice_data = voices[voice]
            model = voice_data["model"]
            speaker = voice_data["speaker"]

            # Handle emotion properly - ensure it's a tensor or None
            emotion = voice_data.get("emotion")
            if emotion is None:
                # Create default emotion tensor with neutral emphasis
                emotion = torch.tensor([
                    0.0,  # happy
                    0.0,  # sad
                    0.0,  # disgust
                    0.0,  # fear
                    0.0,  # surprise
                    0.0,  # anger
                    0.0,  # other
                    1.0   # neutral
                ], device=device, dtype=torch.float32)
            elif not isinstance(emotion, torch.Tensor):
                print("Warning: Invalid emotion format, using neutral")
                emotion = None

            # Add emotion to conditioning if provided
            cond_dict = make_cond_dict(
                text=gen_text,
                speaker=speaker,
                language=voice_data.get("language", language),
                emotion=emotion,
                speaker_noised=voice_data.get("speaker_noised", False)
            )
            
            conditioning = model.prepare_conditioning(cond_dict)

            estimated_generation_duration = 30 * len(text) / 400
            estimated_total_steps = int(estimated_generation_duration * 86)
            pbar = ProgressBar(estimated_total_steps)

            def update_progress(_frame: torch.Tensor, step: int, _total_steps: int) -> bool:
                pbar.update(1)
                return True

            # Generate audio with the parameters
            codes = model.generate(
                prefix_conditioning=conditioning,
                audio_prefix_codes=audio_prefix_codes,
                cfg_scale=cfg_scale,
                sampling_params=dict(min_p=min_p),
                callback=update_progress
            )
            wavs = model.autoencoder.decode(codes).cpu()
            
            generated_audio_segments.append(wavs[0])

        if generated_audio_segments:
            final_wave = torch.cat(generated_audio_segments, dim=-1)
            
        return final_wave, model.autoencoder.sampling_rate

    @classmethod
    def get_supported_models(cls):
        """Get list of supported model types"""
        # Always return both models since we handle downloading
        return cls.model_types

    @classmethod
    def INPUT_TYPES(s):
        """Define input types for the node"""
        return {
            "required": {
                "sample_audio": ("AUDIO",),
                "sample_text": ("STRING", {
                    "multiline": True,
                    "default": "Text of sample_audio"
                }),
                "speech": ("STRING", {
                    "multiline": True,
                    "default": "This is what I want to say"
                }),
                "seed": ("INT", {
                    "display": "number", 
                    "step": 1,
                    "default": 1, 
                    "min": -1,
                    "tooltip": s.tooltip_seed,
                }),
                "model_type": (s.model_types,),
                "language": (supported_language_codes,),
                "cfg_scale": ("FLOAT", {
                    "default": 2.0,
                    "min": 1.0,
                    "max": 5.0,
                    "step": 0.1
                }),
                "min_p": ("FLOAT", {
                    "default": 0.15,
                    "min": 0.0,
                    "max": 1.0,
                    "step": 0.01
                }),
                "speed": ("FLOAT", {
                    "default": 1.0,
                    "tooltip": s.tooltip_speed,
                }),
                "disable_compiler": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "Disable PyTorch compiler for better compatibility"
                }),
            },
            "optional": {
                "prefix_audio": ("AUDIO", {
                    "tooltip": "Optional audio to continue from"
                }),
                "speaker_noised": ("BOOLEAN", {
                    "default": False,
                    "tooltip": "Apply denoising to speaker reference"
                }),
                "emotion": ("EMOTION",),
            }
        }

    CATEGORY = "audio"
    RETURN_TYPES = ("AUDIO",)
    FUNCTION = "create_audio"

    def disable_torch_compiler(self):
        """Disable PyTorch's inductor compiler"""
        torch._dynamo.config.suppress_errors = True
        torch._inductor.config.fallback_random = True
        torch.backends.cudnn.enabled = True
        torch._dynamo.config.automatic_dynamic_shapes = False
        torch.set_float32_matmul_precision('high')

    def create_audio(self, sample_audio, sample_text, speech, seed=-1, 
                    model_type="Zyphra/Zonos-v0.1-transformer", 
                    language="en-us", cfg_scale=2.0, min_p=0.15,
                    speed=1.0, disable_compiler=True, prefix_audio=None, 
                    speaker_noised=False, emotion=None):        
        try:
            # Only disable compiler if explicitly requested
            if disable_compiler:
                self.disable_torch_compiler()

            # Check espeak installation before proceeding
            espeak_path = check_espeak_installation()
            if not espeak_path:
                raise RuntimeError("espeak is not installed or not found. Please install espeak-ng:\n"
                                 "1. Download from https://github.com/espeak-ng/espeak-ng/releases\n"
                                 "2. Run the installer\n"
                                 "3. Restart your computer\n"
                                 "If already installed, ensure it's in your system PATH")
            
            # Set environment variable for phonemizer
            os.environ["PHONEMIZER_ESPEAK_PATH"] = espeak_path

            # Create temporary wav file
            wave_file = tempfile.NamedTemporaryFile(suffix=".wav", delete=False)
            wave_file_name = wave_file.name
            wave_file.close()

            # Process sample audio and save to temp file
            hasAudio = False
            for (batch_number, waveform) in enumerate(sample_audio["waveform"].cpu()):
                # Compute hash of audio content
                audio_hash = self.hash_audio(waveform, sample_audio["sample_rate"])
                
                buff = io.BytesIO()
                torchaudio.save(buff, waveform, sample_audio["sample_rate"], format="WAV")
                with open(wave_file_name, 'wb') as f:
                    f.write(buff.getbuffer())
                hasAudio = True
                break
                
            if not hasAudio:
                raise FileNotFoundError("No audio input")

            # Get model and config paths
            model_path, config_path = self.get_model_path(model_type)
            
            # Cache model loading
            if self.CURRENT_MODEL_TYPE != model_type:
                # TODO: Use model = mm.load_model(model, device)

                if self.CURRENT_MODEL is not None:
                    del self.CURRENT_MODEL
                    torch.cuda.empty_cache()
                print(f"Loading {model_type} model...")
                self.CURRENT_MODEL = Zonos.from_local(
                    model_path=model_path,
                    config_path=config_path, 
                    device=device,
                    backbone="torch"
                )
                self.CURRENT_MODEL_TYPE = model_type
                print(f"{model_type} model loaded successfully!")
            
            model = self.CURRENT_MODEL

            # Improve speaker embedding caching logic using content hash
            if audio_hash != self.CURRENT_SPEAKER_HASH:
                print("Recomputing speaker embedding...")
                wav, sampling_rate = torchaudio.load(wave_file_name)
                self.CURRENT_SPEAKER_EMBEDDING = model.make_speaker_embedding(wav, sampling_rate)
                self.CURRENT_SPEAKER_EMBEDDING = self.CURRENT_SPEAKER_EMBEDDING.to(device, dtype=torch.bfloat16)
                self.CURRENT_SPEAKER_HASH = audio_hash
            
            speaker = self.CURRENT_SPEAKER_EMBEDDING
            
            main_voice = {
                "speaker": speaker,
                "language": language,
                "model": model,
                "emotion": emotion,
                "speaker_noised": speaker_noised
            }
            
            voices = {'main': main_voice}
            chunks = self.split_text(speech)

            # Process prefix audio if provided
            audio_prefix_codes = None
            if prefix_audio is not None:
                wav_prefix = prefix_audio["waveform"].mean(0, keepdim=True)
                wav_prefix = model.autoencoder.preprocess(wav_prefix, prefix_audio["sample_rate"])
                wav_prefix = wav_prefix.to(device, dtype=torch.float32)
                audio_prefix_codes = model.autoencoder.encode(wav_prefix.unsqueeze(0))

            # Generate audio with all parameters
            waveform, sample_rate = self.generate_audio(
                voices, chunks, seed, language,
                cfg_scale=cfg_scale,
                min_p=min_p,
                audio_prefix_codes=audio_prefix_codes
            )

            # Apply speed adjustment if needed
            if speed != 1:
                audio = {"waveform": waveform.unsqueeze(0), "sample_rate": sample_rate}
                audio = self.time_shift(audio, speed)
            else:
                audio = {"waveform": waveform.unsqueeze(0), "sample_rate": sample_rate}

        finally:
            if wave_file_name is not None:
                try:
                    os.unlink(wave_file_name)
                except Exception as e:
                    print("Zonos: Cannot remove? "+wave_file_name)
                    print(e)

        return (audio,)

    @classmethod
    def IS_CHANGED(s, sample_audio, sample_text, speech, seed, model_type, language, 
                   cfg_scale, min_p, speed, disable_compiler=True, prefix_audio=None, 
                   speaker_noised=False, emotion=None):
        """Calculate hash for caching based on all input parameters"""
        m = hashlib.sha256()
        m.update(sample_text.encode())
        m.update(str(sample_audio).encode())
        m.update(speech.encode())
        m.update(str(seed).encode())
        m.update(model_type.encode())
        m.update(language.encode())
        m.update(str(cfg_scale).encode())
        m.update(str(min_p).encode())
        m.update(str(speed).encode())
        m.update(str(disable_compiler).encode())
        if prefix_audio is not None:
            m.update(str(prefix_audio).encode())
        m.update(str(speaker_noised).encode())
        if emotion is not None:
            m.update(str(emotion).encode())
        return m.digest().hex()

    def time_shift(self, audio, speed):
        import torch.nn.functional as F
        rate = audio['sample_rate']
        waveform = audio['waveform']
        
        # Calculate new length
        old_length = waveform.shape[-1]
        new_length = int(old_length / speed)
        
        # Resample audio
        new_waveform = F.interpolate(
            waveform.unsqueeze(1),
            size=new_length,
            mode='linear',
            align_corners=False
        ).squeeze(1)

        return {"waveform": new_waveform, "sample_rate": rate}

class ZonosEmotion:
    """Node for creating emotion vectors for Zonos TTS"""
    
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "happy": ("FLOAT", {
                    "default": 1.0,
                    "min": 0.0,
                    "max": 1.0,
                    "step": 0.05,
                    "tooltip": "Happiness intensity"
                }),
                "sad": ("FLOAT", {
                    "default": 0.05,
                    "min": 0.0,
                    "max": 1.0,
                    "step": 0.05,
                    "tooltip": "Sadness intensity"
                }),
                "disgust": ("FLOAT", {
                    "default": 0.05,
                    "min": 0.0,
                    "max": 1.0,
                    "step": 0.05,
                    "tooltip": "Disgust intensity"
                }),
                "fear": ("FLOAT", {
                    "default": 0.05,
                    "min": 0.0,
                    "max": 1.0,
                    "step": 0.05,
                    "tooltip": "Fear intensity"
                }),
                "surprise": ("FLOAT", {
                    "default": 0.05,
                    "min": 0.0,
                    "max": 1.0,
                    "step": 0.05,
                    "tooltip": "Surprise intensity"
                }),
                "anger": ("FLOAT", {
                    "default": 0.05,
                    "min": 0.0,
                    "max": 1.0,
                    "step": 0.05,
                    "tooltip": "Anger intensity"
                }),
                "other": ("FLOAT", {
                    "default": 0.1,
                    "min": 0.0,
                    "max": 1.0,
                    "step": 0.05,
                    "tooltip": "Other emotion intensity"
                }),
                "neutral": ("FLOAT", {
                    "default": 0.2,
                    "min": 0.0,
                    "max": 1.0,
                    "step": 0.05,
                    "tooltip": "Neutral intensity"
                }),
            }
        }

    CATEGORY = "audio"
    RETURN_TYPES = ("EMOTION",)
    FUNCTION = "create_emotion"

    def create_emotion(self, happy, sad, disgust, fear, surprise, anger, other, neutral):
        # Normalize values to ensure they sum to 1.0
        total = happy + sad + disgust + fear + surprise + anger + other + neutral
        if total > 0:
            happy = happy / total
            sad = sad / total
            disgust = disgust / total
            fear = fear / total
            surprise = surprise / total
            anger = anger / total
            other = other / total
            neutral = neutral / total

        # Create tensor in exact same order as gradio interface
        emotion_tensor = torch.tensor([
            happy,      # e1 - Happiness
            sad,        # e2 - Sadness
            disgust,    # e3 - Disgust
            fear,       # e4 - Fear
            surprise,   # e5 - Surprise
            anger,      # e6 - Anger
            other,      # e7 - Other
            neutral     # e8 - Neutral
        ], device=device, dtype=torch.float32)  # Add explicit dtype to match gradio
        
        return (emotion_tensor,)

    @classmethod
    def IS_CHANGED(s, happy, sad, disgust, fear, surprise, anger, other, neutral):
        m = hashlib.sha256()
        for val in [happy, sad, disgust, fear, surprise, anger, other, neutral]:
            m.update(str(val).encode())
        return m.digest().hex()
    
# EOF

================
File: zonos/autoencoder.py
================
import math

import torch
import torchaudio
from transformers.models.dac import DacModel


class DACAutoencoder:
    def __init__(self):
        super().__init__()
        self.dac = DacModel.from_pretrained("descript/dac_44khz")
        self.dac.eval().requires_grad_(False)
        self.codebook_size = self.dac.config.codebook_size
        self.num_codebooks = self.dac.quantizer.n_codebooks
        self.sampling_rate = self.dac.config.sampling_rate

    def preprocess(self, wav: torch.Tensor, sr: int) -> torch.Tensor:
        wav = torchaudio.functional.resample(wav, sr, 44_100)
        right_pad = math.ceil(wav.shape[-1] / 512) * 512 - wav.shape[-1]
        return torch.nn.functional.pad(wav, (0, right_pad))

    def encode(self, wav: torch.Tensor) -> torch.Tensor:
        return self.dac.encode(wav).audio_codes

    def decode(self, codes: torch.Tensor) -> torch.Tensor:
        with torch.autocast(self.dac.device.type, torch.float16, enabled=self.dac.device.type != "cpu"):
            return self.dac.decode(audio_codes=codes).audio_values.unsqueeze(1).float()

================
File: zonos/backbone/__init__.py
================
BACKBONES = {}

try:
    from ._mamba_ssm import MambaSSMZonosBackbone
    BACKBONES["mamba_ssm"] = MambaSSMZonosBackbone
except ImportError:
    pass

from ._torch import TorchZonosBackbone
BACKBONES["torch"] = TorchZonosBackbone

================
File: zonos/backbone/_mamba_ssm.py
================
import torch
import torch.nn as nn
from mamba_ssm.models.mixer_seq_simple import create_block
from mamba_ssm.ops.triton.layer_norm import layer_norm_fn

from ..config import BackboneConfig, InferenceParams


class MambaSSMZonosBackbone(nn.Module):
    supported_architectures = ["transformer", "hybrid"]

    def __init__(self, config: BackboneConfig):
        super().__init__()
        self.config = config

        self.layers = nn.ModuleList(
            [
                create_block(
                    d_model=config.d_model,
                    d_intermediate=config.d_intermediate
                    if (i not in config.attn_layer_idx)
                    else config.attn_mlp_d_intermediate,
                    ssm_cfg=config.ssm_cfg,
                    layer_idx=i,
                    attn_layer_idx=config.attn_layer_idx,
                    attn_cfg=config.attn_cfg,
                    norm_epsilon=config.norm_epsilon,
                    residual_in_fp32=config.residual_in_fp32,
                    fused_add_norm=True,
                    rms_norm=config.rms_norm,
                )
                for i in range(config.n_layer)
            ]
        )

        self.norm_f = nn.LayerNorm(config.d_model, eps=config.norm_epsilon)

    def allocate_inference_cache(self, batch_size: int, max_seqlen: int, dtype: torch.dtype = torch.bfloat16):
        return {
            i: layer.allocate_inference_cache(batch_size, max_seqlen, dtype=dtype)
            for i, layer in enumerate(self.layers)
        }

    def forward(self, hidden_states: torch.Tensor, inference_params: InferenceParams | None = None):
        residual = None
        for layer in self.layers:
            hidden_states, residual = layer(hidden_states, residual, inference_params)

        return layer_norm_fn(
            hidden_states,
            self.norm_f.weight,
            self.norm_f.bias,
            residual,
            eps=self.norm_f.eps,
            residual_in_fp32=self.config.residual_in_fp32,
            is_rms_norm=self.config.rms_norm,
        )

================
File: zonos/backbone/_torch.py
================
# Based on gpt-fast: https://github.com/pytorch-labs/gpt-fast/blob/095b2229ee3a40e379c11f05b94bd6923db63b4b/model.py
import torch
import torch.nn as nn
from torch.nn import functional as F

from ..config import BackboneConfig, InferenceParams


def precompute_freqs_cis(seq_len: int, n_elem: int, base: float = 10000) -> torch.Tensor:
    freqs = 1.0 / (base ** (torch.arange(0, n_elem, 2)[: (n_elem // 2)].float() / n_elem))
    t = torch.arange(seq_len, device=freqs.device)
    freqs = torch.outer(t, freqs)
    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)
    cache = torch.stack([freqs_cis.real, freqs_cis.imag], dim=-1)
    return cache


def apply_rotary_emb(x: torch.Tensor, freqs_cis: torch.Tensor) -> torch.Tensor:
    xshaped = x.float().reshape(*x.shape[:-1], -1, 2)
    freqs_cis = freqs_cis.view(-1, xshaped.size(1), 1, xshaped.size(3), 2)
    x_out2 = torch.stack(
        [
            xshaped[..., 0] * freqs_cis[..., 0] - xshaped[..., 1] * freqs_cis[..., 1],
            xshaped[..., 1] * freqs_cis[..., 0] + xshaped[..., 0] * freqs_cis[..., 1],
        ],
        -1,
    )

    x_out2 = x_out2.flatten(3)
    return x_out2.type_as(x)


def _update_kv_cache(
    k: torch.Tensor, v: torch.Tensor, inference_params: InferenceParams, layer_idx: int
) -> torch.Tensor:
    """k/v: (batch_size, seqlen, nheads, head_dim) or (batch_size, 1, nheads, head_dim)"""
    assert layer_idx in inference_params.key_value_memory_dict
    kv_cache, _ = inference_params.key_value_memory_dict[layer_idx]
    # Adjust key and value for inference
    batch_start = inference_params.batch_size_offset
    batch_end = batch_start + k.shape[0]
    sequence_start = inference_params.seqlen_offset
    sequence_end = sequence_start + k.shape[1]
    assert batch_end <= kv_cache.shape[0]
    assert sequence_end <= kv_cache.shape[1]
    assert kv_cache is not None
    kv_cache[batch_start:batch_end, sequence_start:sequence_end, 0, ...] = k
    kv_cache[batch_start:batch_end, sequence_start:sequence_end, 1, ...] = v
    return kv_cache[batch_start:batch_end, :sequence_end, ...]


class TorchZonosBackbone(nn.Module):
    supported_architectures = ["transformer"]
    freqs_cis: torch.Tensor

    def __init__(self, config: BackboneConfig):
        assert not config.ssm_cfg, "This backbone implementation only supports the Transformer model."
        super().__init__()
        self.config = config

        self.layers = nn.ModuleList(TransformerBlock(config, i) for i in range(config.n_layer))
        self.norm_f = nn.LayerNorm(config.d_model, eps=config.norm_epsilon)

    def allocate_inference_cache(self, batch_size: int, max_seqlen: int, dtype: torch.dtype = torch.bfloat16):
        # TODO: This function should be pure
        head_dim = self.config.d_model // self.config.attn_cfg["num_heads"]
        self.freqs_cis = precompute_freqs_cis(16384, head_dim)
        return {
            i: layer.allocate_inference_cache(batch_size, max_seqlen, dtype=dtype)
            for i, layer in enumerate(self.layers)
        }

    def forward(self, hidden_states: torch.Tensor, inference_params: InferenceParams) -> torch.Tensor:
        input_pos = torch.arange(0, hidden_states.shape[1], device=hidden_states.device)
        input_pos = input_pos + inference_params.lengths_per_sample.unsqueeze(-1)

        freqs_cis = self.freqs_cis[input_pos].expand(hidden_states.shape[0], -1, -1, -1)
        for i, layer in enumerate(self.layers):
            hidden_states = layer(hidden_states, inference_params, freqs_cis)
        return self.norm_f(hidden_states)


class TransformerBlock(nn.Module):
    def __init__(self, config: BackboneConfig, layer_idx: int) -> None:
        super().__init__()
        self.config = config

        self.norm = nn.LayerNorm(config.d_model, eps=config.norm_epsilon)
        self.mixer = Attention(config, layer_idx)
        self.norm2 = nn.LayerNorm(config.d_model, eps=config.norm_epsilon)
        self.mlp = FeedForward(config)

        self.num_heads_kv = config.attn_cfg["num_heads_kv"]
        self.head_dim = config.d_model // config.attn_cfg["num_heads"]

    def allocate_inference_cache(self, batch_size: int, max_seqlen: int, dtype: torch.dtype = torch.bfloat16):
        return torch.empty(batch_size, max_seqlen, 2, self.num_heads_kv, self.head_dim, dtype=dtype), None

    def forward(self, x: torch.Tensor, inference_params: InferenceParams, freqs_cis: torch.Tensor) -> torch.Tensor:
        x = x + self.mixer(self.norm(x), inference_params, freqs_cis)
        x = x + self.mlp(self.norm2(x))
        return x


class Attention(nn.Module):
    def __init__(self, config: BackboneConfig, layer_idx: int):
        super().__init__()
        self.num_heads = config.attn_cfg["num_heads"]
        self.num_heads_kv = config.attn_cfg["num_heads_kv"]
        self.head_dim = config.d_model // self.num_heads
        self.layer_idx = layer_idx

        total_head_dim = (self.num_heads + 2 * self.num_heads_kv) * self.head_dim
        self.in_proj = nn.Linear(config.d_model, total_head_dim, bias=False)
        self.out_proj = nn.Linear(self.num_heads * self.head_dim, config.d_model, bias=False)

    def forward(self, x: torch.Tensor, inference_params: InferenceParams, freqs_cis: torch.Tensor) -> torch.Tensor:
        batch_size, seqlen, _ = x.shape

        q_size = self.num_heads * self.head_dim
        kv_size = self.num_heads_kv * self.head_dim
        q, k, v = self.in_proj(x).split([q_size, kv_size, kv_size], dim=-1)

        q = q.view(batch_size, seqlen, self.num_heads, self.head_dim)
        k = k.view(batch_size, seqlen, self.num_heads_kv, self.head_dim)
        v = v.view(batch_size, seqlen, self.num_heads_kv, self.head_dim)

        q = apply_rotary_emb(q, freqs_cis)
        k = apply_rotary_emb(k, freqs_cis)

        kv = _update_kv_cache(k, v, inference_params, self.layer_idx)
        k, v = kv.unbind(dim=-3)

        q, k, v = map(lambda x: x.transpose(1, 2), (q, k, v))

        y = F.scaled_dot_product_attention(q, k, v, is_causal=seqlen > 1, enable_gqa=True)

        y = y.transpose(1, 2).contiguous().view(batch_size, seqlen, q_size)

        y = self.out_proj(y)
        return y


class FeedForward(nn.Module):
    def __init__(self, config: BackboneConfig) -> None:
        super().__init__()
        self.fc1 = nn.Linear(config.d_model, 2 * config.attn_mlp_d_intermediate, bias=False)
        self.fc2 = nn.Linear(config.attn_mlp_d_intermediate, config.d_model, bias=False)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        y, gate = self.fc1(x).chunk(2, dim=-1)
        return self.fc2(y * F.silu(gate))

================
File: zonos/codebook_pattern.py
================
import torch
import torch.nn.functional as F


def apply_delay_pattern(codes: torch.Tensor, mask_token: int):
    codes = F.pad(codes, (0, codes.shape[1]), value=mask_token)
    return torch.stack([codes[:, k].roll(k + 1) for k in range(codes.shape[1])], dim=1)


def revert_delay_pattern(codes: torch.Tensor):
    _, n_q, seq_len = codes.shape
    return torch.stack([codes[:, k, k + 1 : seq_len - n_q + k + 1] for k in range(n_q)], dim=1)

================
File: zonos/conditioning.py
================
from functools import cache
from typing import Any, Literal, Iterable

import torch
import torch.nn as nn

from .config import PrefixConditionerConfig
from .utils import DEFAULT_DEVICE


class Conditioner(nn.Module):
    def __init__(
        self,
        output_dim: int,
        name: str,
        cond_dim: int | None = None,
        projection: Literal["none", "linear", "mlp"] = "none",
        uncond_type: Literal["learned", "none"] = "none",
        **kwargs,
    ):
        super().__init__()
        self.name = name
        self.output_dim = output_dim
        self.cond_dim = cond_dim = cond_dim or output_dim

        if projection == "linear":
            self.project = nn.Linear(cond_dim, output_dim)
        elif projection == "mlp":
            self.project = nn.Sequential(
                nn.Linear(cond_dim, output_dim),
                nn.SiLU(),
                nn.Linear(output_dim, output_dim),
            )
        else:
            self.project = nn.Identity()

        self.uncond_vector = None
        if uncond_type == "learned":
            self.uncond_vector = nn.Parameter(torch.zeros(output_dim))

    def apply_cond(self, *inputs: Any) -> torch.Tensor:
        raise NotImplementedError()

    def forward(self, inputs: tuple[Any, ...] | None) -> torch.Tensor:
        if inputs is None:
            assert self.uncond_vector is not None
            return self.uncond_vector.data.view(1, 1, -1)

        cond = self.apply_cond(*inputs)
        cond = self.project(cond)
        return cond


# ------- ESPEAK CONTAINMENT ZONE ------------------------------------------------------------------------------------------------------------------------------------------------
import os
import sys
import re
import unicodedata

import inflect
import torch
import torch.nn as nn
from kanjize import number2kanji
from phonemizer.backend import EspeakBackend
from sudachipy import Dictionary, SplitMode

if sys.platform == "darwin":
    os.environ["PHONEMIZER_ESPEAK_LIBRARY"] = "/opt/homebrew/lib/libespeak-ng.dylib"

# --- Number normalization code from https://github.com/daniilrobnikov/vits2/blob/main/text/normalize_numbers.py ---

_inflect = inflect.engine()
_comma_number_re = re.compile(r"([0-9][0-9\,]+[0-9])")
_decimal_number_re = re.compile(r"([0-9]+\.[0-9]+)")
_pounds_re = re.compile(r"Â£([0-9\,]*[0-9]+)")
_dollars_re = re.compile(r"\$([0-9\.\,]*[0-9]+)")
_ordinal_re = re.compile(r"[0-9]+(st|nd|rd|th)")
_number_re = re.compile(r"[0-9]+")


def _remove_commas(m: re.Match) -> str:
    return m.group(1).replace(",", "")


def _expand_decimal_point(m: re.Match) -> str:
    return m.group(1).replace(".", " point ")


def _expand_dollars(m: re.Match) -> str:
    match = m.group(1)
    parts = match.split(".")
    if len(parts) > 2:
        return match + " dollars"  # Unexpected format
    dollars = int(parts[0]) if parts[0] else 0
    cents = int(parts[1]) if len(parts) > 1 and parts[1] else 0
    if dollars and cents:
        dollar_unit = "dollar" if dollars == 1 else "dollars"
        cent_unit = "cent" if cents == 1 else "cents"
        return "%s %s, %s %s" % (dollars, dollar_unit, cents, cent_unit)
    elif dollars:
        dollar_unit = "dollar" if dollars == 1 else "dollars"
        return "%s %s" % (dollars, dollar_unit)
    elif cents:
        cent_unit = "cent" if cents == 1 else "cents"
        return "%s %s" % (cents, cent_unit)
    else:
        return "zero dollars"


def _expand_ordinal(m: re.Match) -> str:
    return _inflect.number_to_words(m.group(0))


def _expand_number(m: re.Match) -> str:
    num = int(m.group(0))
    if num > 1000 and num < 3000:
        if num == 2000:
            return "two thousand"
        elif num > 2000 and num < 2010:
            return "two thousand " + _inflect.number_to_words(num % 100)
        elif num % 100 == 0:
            return _inflect.number_to_words(num // 100) + " hundred"
        else:
            return _inflect.number_to_words(num, andword="", zero="oh", group=2).replace(", ", " ")
    else:
        return _inflect.number_to_words(num, andword="")


def normalize_numbers(text: str) -> str:
    text = re.sub(_comma_number_re, _remove_commas, text)
    text = re.sub(_pounds_re, r"\1 pounds", text)
    text = re.sub(_dollars_re, _expand_dollars, text)
    text = re.sub(_decimal_number_re, _expand_decimal_point, text)
    text = re.sub(_ordinal_re, _expand_ordinal, text)
    text = re.sub(_number_re, _expand_number, text)
    return text


# --- Number normalization code end ---


PAD_ID, UNK_ID, BOS_ID, EOS_ID = 0, 1, 2, 3
SPECIAL_TOKEN_IDS = [PAD_ID, UNK_ID, BOS_ID, EOS_ID]

_punctuation = ';:,.!?Â¡Â¿â€”â€¦"Â«Â»â€œâ€() *~-/\\&'
_letters = "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz"
_letters_ipa = (
    "É‘ÉÉ’Ã¦É“Ê™Î²É”É•Ã§É—É–Ã°Ê¤É™É˜ÉšÉ›ÉœÉÉžÉŸÊ„É¡É É¢Ê›É¦É§Ä§É¥ÊœÉ¨ÉªÊÉ­É¬É«É®ÊŸÉ±É¯É°Å‹É³É²É´Ã¸ÉµÉ¸Î¸Å“É¶Ê˜É¹ÉºÉ¾É»Ê€ÊÉ½Ê‚ÊƒÊˆÊ§Ê‰ÊŠÊ‹â±±ÊŒÉ£É¤ÊÏ‡ÊŽÊÊ‘ÊÊ’Ê”Ê¡Ê•Ê¢Ç€ÇÇ‚ÇƒËˆËŒËË‘Ê¼Ê´Ê°Ê±Ê²Ê·Ë Ë¤Ëžâ†“â†‘â†’â†—â†˜'Ì©'áµ»"
)

symbols = [*_punctuation, *_letters, *_letters_ipa]
_symbol_to_id = {s: i for i, s in enumerate(symbols, start=len(SPECIAL_TOKEN_IDS))}


def _get_symbol_id(s: str) -> int:
    return _symbol_to_id.get(s, 1)


def get_symbol_ids(text: str) -> list[int]:
    return list(map(_get_symbol_id, text))


def tokenize_phonemes(phonemes: list[str]) -> tuple[torch.Tensor, list[int]]:
    phoneme_ids = [[BOS_ID, *get_symbol_ids(phonemes), EOS_ID] for phonemes in phonemes]
    lengths = list(map(len, phoneme_ids))
    longest = max(lengths)
    phoneme_ids = [[PAD_ID] * (longest - len(ids)) + ids for ids in phoneme_ids]
    return torch.tensor(phoneme_ids), lengths


def normalize_jp_text(text: str, tokenizer=Dictionary(dict="full").create()) -> str:
    text = unicodedata.normalize("NFKC", text)
    text = re.sub(r"\d+", lambda m: number2kanji(int(m[0])), text)
    final_text = " ".join([x.reading_form() for x in tokenizer.tokenize(text, SplitMode.A)])
    return final_text


def clean(texts: list[str], languages: list[str]) -> list[str]:
    texts_out = []
    for text, language in zip(texts, languages):
        if "ja" in language:
            text = normalize_jp_text(text)
        else:
            text = normalize_numbers(text)
        texts_out.append(text)
    return texts_out


@cache
def get_backend(language: str) -> "EspeakBackend":
    import logging

    from phonemizer.backend import EspeakBackend

    logger = logging.getLogger("phonemizer")
    backend = EspeakBackend(
        language,
        preserve_punctuation=True,
        with_stress=True,
        punctuation_marks=_punctuation,
        logger=logger,
    )
    logger.setLevel(logging.ERROR)
    return backend


def phonemize(texts: list[str], languages: list[str]) -> list[str]:
    texts = clean(texts, languages)

    batch_phonemes = []
    for text, language in zip(texts, languages):
        backend = get_backend(language)
        phonemes = backend.phonemize([text], strip=True)
        batch_phonemes.append(phonemes[0])

    return batch_phonemes


class EspeakPhonemeConditioner(Conditioner):
    def __init__(self, output_dim: int, **kwargs):
        super().__init__(output_dim, **kwargs)
        self.phoneme_embedder = nn.Embedding(len(SPECIAL_TOKEN_IDS) + len(symbols), output_dim)

    def apply_cond(self, texts: list[str], languages: list[str]) -> torch.Tensor:
        """
        Args:
            texts: list of texts to convert to phonemes
            languages: ISO 639-1 -or otherwise eSpeak compatible- language code
        """
        device = self.phoneme_embedder.weight.device

        phonemes = phonemize(texts, languages)
        phoneme_ids, _ = tokenize_phonemes(phonemes)
        phoneme_embeds = self.phoneme_embedder(phoneme_ids.to(device))

        return phoneme_embeds


# ------- ESPEAK CONTAINMENT ZONE ------------------------------------------------------------------------------------------------------------------------------------------------


class FourierConditioner(Conditioner):
    def __init__(
        self,
        output_dim: int,
        input_dim: int = 1,
        std: float = 1.0,
        min_val: float = 0.0,
        max_val: float = 1.0,
        **kwargs,
    ):
        assert output_dim % 2 == 0
        super().__init__(output_dim, **kwargs)
        self.register_buffer("weight", torch.randn([output_dim // 2, input_dim]) * std)
        self.input_dim, self.min_val, self.max_val = input_dim, min_val, max_val

    def apply_cond(self, x: torch.Tensor) -> torch.Tensor:
        assert x.shape[-1] == self.input_dim
        x = (x - self.min_val) / (self.max_val - self.min_val)  # [batch_size, seq_len, input_dim]
        f = 2 * torch.pi * x.to(self.weight.dtype) @ self.weight.T  # [batch_size, seq_len, output_dim // 2]
        return torch.cat([f.cos(), f.sin()], dim=-1)  # [batch_size, seq_len, output_dim]


class IntegerConditioner(Conditioner):
    def __init__(self, output_dim: int, min_val: int = 0, max_val: int = 512, **kwargs):
        super().__init__(output_dim, **kwargs)
        self.min_val = min_val
        self.max_val = max_val
        self.int_embedder = nn.Embedding(max_val - min_val + 1, output_dim)

    def apply_cond(self, x: torch.Tensor) -> torch.Tensor:
        assert x.shape[-1] == 1
        return self.int_embedder(x.squeeze(-1) - self.min_val)  # [batch_size, seq_len, output_dim]


class PassthroughConditioner(Conditioner):
    def __init__(self, output_dim: int, **kwargs):
        super().__init__(output_dim, **kwargs)

    def apply_cond(self, x: torch.Tensor) -> torch.Tensor:
        assert x.shape[-1] == self.cond_dim
        return x


_cond_cls_map = {
    "PassthroughConditioner": PassthroughConditioner,
    "EspeakPhonemeConditioner": EspeakPhonemeConditioner,
    "FourierConditioner": FourierConditioner,
    "IntegerConditioner": IntegerConditioner,
}


def build_conditioners(conditioners: list[dict], output_dim: int) -> list[Conditioner]:
    return [_cond_cls_map[config["type"]](output_dim, **config) for config in conditioners]


class PrefixConditioner(Conditioner):
    def __init__(self, config: PrefixConditionerConfig, output_dim: int):
        super().__init__(output_dim, "prefix", projection=config.projection)
        self.conditioners = nn.ModuleList(build_conditioners(config.conditioners, output_dim))
        self.norm = nn.LayerNorm(output_dim)
        self.required_keys = {c.name for c in self.conditioners if c.uncond_vector is None}

    def forward(self, cond_dict: dict) -> torch.Tensor:
        if not set(cond_dict).issuperset(self.required_keys):
            raise ValueError(f"Missing required keys: {self.required_keys - set(cond_dict)}")
        conds = []
        for conditioner in self.conditioners:
            conds.append(conditioner(cond_dict.get(conditioner.name)))
        max_bsz = max(map(len, conds))
        assert all(c.shape[0] in (max_bsz, 1) for c in conds)
        conds = [c.expand(max_bsz, -1, -1) for c in conds]
        return self.norm(self.project(torch.cat(conds, dim=-2)))


supported_language_codes = [
    'af', 'am', 'an', 'ar', 'as', 'az', 'ba', 'bg', 'bn', 'bpy', 'bs', 'ca', 'cmn',
    'cs', 'cy', 'da', 'de', 'el', 'en-029', 'en-gb', 'en-gb-scotland', 'en-gb-x-gbclan',
    'en-gb-x-gbcwmd', 'en-gb-x-rp', 'en-us', 'eo', 'es', 'es-419', 'et', 'eu', 'fa',
    'fa-latn', 'fi', 'fr-be', 'fr-ch', 'fr-fr', 'ga', 'gd', 'gn', 'grc', 'gu', 'hak',
    'hi', 'hr', 'ht', 'hu', 'hy', 'hyw', 'ia', 'id', 'is', 'it', 'ja', 'jbo', 'ka',
    'kk', 'kl', 'kn', 'ko', 'kok', 'ku', 'ky', 'la', 'lfn', 'lt', 'lv', 'mi', 'mk',
    'ml', 'mr', 'ms', 'mt', 'my', 'nb', 'nci', 'ne', 'nl', 'om', 'or', 'pa', 'pap',
    'pl', 'pt', 'pt-br', 'py', 'quc', 'ro', 'ru', 'ru-lv', 'sd', 'shn', 'si', 'sk',
    'sl', 'sq', 'sr', 'sv', 'sw', 'ta', 'te', 'tn', 'tr', 'tt', 'ur', 'uz', 'vi',
    'vi-vn-x-central', 'vi-vn-x-south', 'yue'
]  # fmt: off


def make_cond_dict(
    text: str = "It would be nice to have time for testing, indeed.",
    language: str = "en-us",
    speaker: torch.Tensor | None = None,
    emotion: list[float] = [0.3077, 0.0256, 0.0256, 0.0256, 0.0256, 0.0256, 0.2564, 0.3077],
    fmax: float = 22050.0,
    pitch_std: float = 20.0,
    speaking_rate: float = 15.0,
    vqscore_8: list[float] = [0.78] * 8,
    ctc_loss: float = 0.0,
    dnsmos_ovrl: float = 4.0,
    speaker_noised: bool = False,
    unconditional_keys: Iterable[str] = {"vqscore_8", "dnsmos_ovrl"},
    device: torch.device | str = DEFAULT_DEVICE,
) -> dict:
    """
    A helper to build the 'cond_dict' that the model expects.
    By default, it will generate a random speaker embedding
    """
    assert language.lower() in supported_language_codes, "Please pick a supported language"

    language_code_to_id = {lang: i for i, lang in enumerate(supported_language_codes)}

    cond_dict = {
        "espeak": ([text], [language]),
        "speaker": speaker,
        "emotion": emotion,
        "fmax": fmax,
        "pitch_std": pitch_std,
        "speaking_rate": speaking_rate,
        "language_id": language_code_to_id[language],
        "vqscore_8": vqscore_8,
        "ctc_loss": ctc_loss,
        "dnsmos_ovrl": dnsmos_ovrl,
        "speaker_noised": int(speaker_noised),
    }

    for k in unconditional_keys:
        cond_dict.pop(k, None)

    for k, v in cond_dict.items():
        if isinstance(v, (float, int, list)):
            v = torch.tensor(v)
        if isinstance(v, torch.Tensor):
            cond_dict[k] = v.view(1, 1, -1).to(device)

        if k == "emotion":
            cond_dict[k] /= cond_dict[k].sum(dim=-1)

    return cond_dict

================
File: zonos/config.py
================
from dataclasses import dataclass, field
from typing import Literal

import torch


# https://github.com/state-spaces/mamba/blob//mamba_ssm/utils/generation.py#L18
@dataclass
class InferenceParams:
    """Inference parameters that are passed to the main model in order
    to efficienly calculate and store the context during inference."""

    max_seqlen: int
    max_batch_size: int
    seqlen_offset: int = 0
    batch_size_offset: int = 0
    key_value_memory_dict: dict = field(default_factory=dict)
    lengths_per_sample: torch.Tensor | None = None

    def reset(self, max_seqlen, max_batch_size):
        self.max_seqlen = max_seqlen
        self.max_batch_size = max_batch_size
        self.seqlen_offset = 0
        if self.lengths_per_sample is not None:
            self.lengths_per_sample.zero_()


@dataclass
class BackboneConfig:
    d_model: int = 1024
    d_intermediate: int = 0
    attn_mlp_d_intermediate: int = 0
    n_layer: int = 16
    ssm_cfg: dict = field(default_factory=dict)
    attn_layer_idx: list = field(default_factory=list)
    attn_cfg: dict = field(default_factory=dict)
    rms_norm: bool = False
    residual_in_fp32: bool = False
    norm_epsilon: float = 1e-5


@dataclass
class PrefixConditionerConfig:
    conditioners: list[dict]
    projection: Literal["none", "linear", "mlp"]


@dataclass
class ZonosConfig:
    backbone: BackboneConfig
    prefix_conditioner: PrefixConditionerConfig
    eos_token_id: int = 1024
    masked_token_id: int = 1025
    pad_vocab_to_multiple_of: int = 8

    @classmethod
    def from_dict(cls, d: dict) -> "ZonosConfig":
        d = d.copy()
        backbone_config = BackboneConfig(**d.pop("backbone"))
        prefix_conditioner_config = PrefixConditionerConfig(**d.pop("prefix_conditioner"))
        config = cls(backbone_config, prefix_conditioner_config, **d)
        return config

================
File: zonos/gradio_interface.py
================
import torch
import torchaudio
import gradio as gr
from os import getenv

from .model import Zonos, DEFAULT_BACKBONE_CLS as ZonosBackbone
from .conditioning import make_cond_dict, supported_language_codes
from .utils import DEFAULT_DEVICE as device

CURRENT_MODEL_TYPE = None
CURRENT_MODEL = None

SPEAKER_EMBEDDING = None
SPEAKER_AUDIO_PATH = None


def load_model_if_needed(model_choice: str):
    global CURRENT_MODEL_TYPE, CURRENT_MODEL
    if CURRENT_MODEL_TYPE != model_choice:
        if CURRENT_MODEL is not None:
            del CURRENT_MODEL
            torch.cuda.empty_cache()
        print(f"Loading {model_choice} model...")
        CURRENT_MODEL = .from_pretrained(model_choice, device=device)
        CURRENT_MODEL.requires_grad_(False).eval()
        CURRENT_MODEL_TYPE = model_choice
        print(f"{model_choice} model loaded successfully!")
    return CURRENT_MODEL


def update_ui(model_choice):
    """
    Dynamically show/hide UI elements based on the model's conditioners.
    We do NOT display 'language_id' or 'ctc_loss' even if they exist in the model.
    """
    model = load_model_if_needed(model_choice)
    cond_names = [c.name for c in model.prefix_conditioner.conditioners]
    print("Conditioners in this model:", cond_names)

    text_update = gr.update(visible=("espeak" in cond_names))
    language_update = gr.update(visible=("espeak" in cond_names))
    speaker_audio_update = gr.update(visible=("speaker" in cond_names))
    prefix_audio_update = gr.update(visible=True)
    emotion1_update = gr.update(visible=("emotion" in cond_names))
    emotion2_update = gr.update(visible=("emotion" in cond_names))
    emotion3_update = gr.update(visible=("emotion" in cond_names))
    emotion4_update = gr.update(visible=("emotion" in cond_names))
    emotion5_update = gr.update(visible=("emotion" in cond_names))
    emotion6_update = gr.update(visible=("emotion" in cond_names))
    emotion7_update = gr.update(visible=("emotion" in cond_names))
    emotion8_update = gr.update(visible=("emotion" in cond_names))
    vq_single_slider_update = gr.update(visible=("vqscore_8" in cond_names))
    fmax_slider_update = gr.update(visible=("fmax" in cond_names))
    pitch_std_slider_update = gr.update(visible=("pitch_std" in cond_names))
    speaking_rate_slider_update = gr.update(visible=("speaking_rate" in cond_names))
    dnsmos_slider_update = gr.update(visible=("dnsmos_ovrl" in cond_names))
    speaker_noised_checkbox_update = gr.update(visible=("speaker_noised" in cond_names))
    unconditional_keys_update = gr.update(
        choices=[name for name in cond_names if name not in ("espeak", "language_id")]
    )

    return (
        text_update,
        language_update,
        speaker_audio_update,
        prefix_audio_update,
        emotion1_update,
        emotion2_update,
        emotion3_update,
        emotion4_update,
        emotion5_update,
        emotion6_update,
        emotion7_update,
        emotion8_update,
        vq_single_slider_update,
        fmax_slider_update,
        pitch_std_slider_update,
        speaking_rate_slider_update,
        dnsmos_slider_update,
        speaker_noised_checkbox_update,
        unconditional_keys_update,
    )


def generate_audio(
    model_choice,
    text,
    language,
    speaker_audio,
    prefix_audio,
    e1,
    e2,
    e3,
    e4,
    e5,
    e6,
    e7,
    e8,
    vq_single,
    fmax,
    pitch_std,
    speaking_rate,
    dnsmos_ovrl,
    speaker_noised,
    cfg_scale,
    min_p,
    seed,
    randomize_seed,
    unconditional_keys,
    progress=gr.Progress(),
):
    """
    Generates audio based on the provided UI parameters.
    We do NOT use language_id or ctc_loss even if the model has them.
    """
    selected_model = load_model_if_needed(model_choice)

    speaker_noised_bool = bool(speaker_noised)
    fmax = float(fmax)
    pitch_std = float(pitch_std)
    speaking_rate = float(speaking_rate)
    dnsmos_ovrl = float(dnsmos_ovrl)
    cfg_scale = float(cfg_scale)
    min_p = float(min_p)
    seed = int(seed)
    max_new_tokens = 86 * 30

    # This is a bit ew, but works for now.
    global SPEAKER_AUDIO_PATH, SPEAKER_EMBEDDING

    if randomize_seed:
        seed = torch.randint(0, 2**32 - 1, (1,)).item()
    torch.manual_seed(seed)

    if speaker_audio is not None and "speaker" not in unconditional_keys:
        if speaker_audio != SPEAKER_AUDIO_PATH:
            print("Recomputed speaker embedding")
            wav, sr = torchaudio.load(speaker_audio)
            SPEAKER_EMBEDDING = selected_model.make_speaker_embedding(wav, sr)
            SPEAKER_EMBEDDING = SPEAKER_EMBEDDING.to(device, dtype=torch.bfloat16)
            SPEAKER_AUDIO_PATH = speaker_audio

    audio_prefix_codes = None
    if prefix_audio is not None:
        wav_prefix, sr_prefix = torchaudio.load(prefix_audio)
        wav_prefix = wav_prefix.mean(0, keepdim=True)
        wav_prefix = selected_model.autoencoder.preprocess(wav_prefix, sr_prefix)
        wav_prefix = wav_prefix.to(device, dtype=torch.float32)
        audio_prefix_codes = selected_model.autoencoder.encode(wav_prefix.unsqueeze(0))

    emotion_tensor = torch.tensor(list(map(float, [e1, e2, e3, e4, e5, e6, e7, e8])), device=device)

    vq_val = float(vq_single)
    vq_tensor = torch.tensor([vq_val] * 8, device=device).unsqueeze(0)

    cond_dict = make_cond_dict(
        text=text,
        language=language,
        speaker=SPEAKER_EMBEDDING,
        emotion=emotion_tensor,
        vqscore_8=vq_tensor,
        fmax=fmax,
        pitch_std=pitch_std,
        speaking_rate=speaking_rate,
        dnsmos_ovrl=dnsmos_ovrl,
        speaker_noised=speaker_noised_bool,
        device=device,
        unconditional_keys=unconditional_keys,
    )
    conditioning = selected_model.prepare_conditioning(cond_dict)

    estimated_generation_duration = 30 * len(text) / 400
    estimated_total_steps = int(estimated_generation_duration * 86)

    def update_progress(_frame: torch.Tensor, step: int, _total_steps: int) -> bool:
        progress((step, estimated_total_steps))
        return True

    codes = selected_model.generate(
        prefix_conditioning=conditioning,
        audio_prefix_codes=audio_prefix_codes,
        max_new_tokens=max_new_tokens,
        cfg_scale=cfg_scale,
        batch_size=1,
        sampling_params=dict(min_p=min_p),
        callback=update_progress,
    )

    wav_out = selected_model.autoencoder.decode(codes).cpu().detach()
    sr_out = selected_model.autoencoder.sampling_rate
    if wav_out.dim() == 2 and wav_out.size(0) > 1:
        wav_out = wav_out[0:1, :]
    return (sr_out, wav_out.squeeze().numpy()), seed


def build_interface():
    supported_models = []
    if "transformer" in ZonosBackbone.supported_architectures:
        supported_models.append("Zyphra/Zonos-v0.1-transformer")

    if "hybrid" in ZonosBackbone.supported_architectures:
        supported_models.append("Zyphra/Zonos-v0.1-hybrid")
    else:
        print(
            "| The current ZonosBackbone does not support the hybrid architecture, meaning only the transformer model will be available in the model selector.\n"
            "| This probably means the mamba-ssm library has not been installed."
        )

    with gr.Blocks() as demo:
        with gr.Row():
            with gr.Column():
                model_choice = gr.Dropdown(
                    choices=supported_models,
                    value=supported_models[0],
                    label="Zonos Model Type",
                    info="Select the model variant to use.",
                )
                text = gr.Textbox(
                    label="Text to Synthesize",
                    value="Zonos uses eSpeak for text to phoneme conversion!",
                    lines=4,
                    max_length=500,  # approximately
                )
                language = gr.Dropdown(
                    choices=supported_language_codes,
                    value="en-us",
                    label="Language Code",
                    info="Select a language code.",
                )
            prefix_audio = gr.Audio(
                value="assets/silence_100ms.wav",
                label="Optional Prefix Audio (continue from this audio)",
                type="filepath",
            )
            with gr.Column():
                speaker_audio = gr.Audio(
                    label="Optional Speaker Audio (for cloning)",
                    type="filepath",
                )
                speaker_noised_checkbox = gr.Checkbox(label="Denoise Speaker?", value=False)

        with gr.Row():
            with gr.Column():
                gr.Markdown("## Conditioning Parameters")
                dnsmos_slider = gr.Slider(1.0, 5.0, value=4.0, step=0.1, label="DNSMOS Overall")
                fmax_slider = gr.Slider(0, 24000, value=24000, step=1, label="Fmax (Hz)")
                vq_single_slider = gr.Slider(0.5, 0.8, 0.78, 0.01, label="VQ Score")
                pitch_std_slider = gr.Slider(0.0, 300.0, value=45.0, step=1, label="Pitch Std")
                speaking_rate_slider = gr.Slider(5.0, 30.0, value=15.0, step=0.5, label="Speaking Rate")

            with gr.Column():
                gr.Markdown("## Generation Parameters")
                cfg_scale_slider = gr.Slider(1.0, 5.0, 2.0, 0.1, label="CFG Scale")
                min_p_slider = gr.Slider(0.0, 1.0, 0.15, 0.01, label="Min P")
                seed_number = gr.Number(label="Seed", value=420, precision=0)
                randomize_seed_toggle = gr.Checkbox(label="Randomize Seed (before generation)", value=True)

        with gr.Accordion("Advanced Parameters", open=False):
            gr.Markdown(
                "### Unconditional Toggles\n"
                "Checking a box will make the model ignore the corresponding conditioning value and make it unconditional.\n"
                'Practically this means the given conditioning feature will be unconstrained and "filled in automatically".'
            )
            with gr.Row():
                unconditional_keys = gr.CheckboxGroup(
                    [
                        "speaker",
                        "emotion",
                        "vqscore_8",
                        "fmax",
                        "pitch_std",
                        "speaking_rate",
                        "dnsmos_ovrl",
                        "speaker_noised",
                    ],
                    value=["emotion"],
                    label="Unconditional Keys",
                )

            gr.Markdown(
                "### Emotion Sliders\n"
                "Warning: The way these sliders work is not intuitive and may require some trial and error to get the desired effect.\n"
                "Certain configurations can cause the model to become unstable. Setting emotion to unconditional may help."
            )
            with gr.Row():
                emotion1 = gr.Slider(0.0, 1.0, 1.0, 0.05, label="Happiness")
                emotion2 = gr.Slider(0.0, 1.0, 0.05, 0.05, label="Sadness")
                emotion3 = gr.Slider(0.0, 1.0, 0.05, 0.05, label="Disgust")
                emotion4 = gr.Slider(0.0, 1.0, 0.05, 0.05, label="Fear")
            with gr.Row():
                emotion5 = gr.Slider(0.0, 1.0, 0.05, 0.05, label="Surprise")
                emotion6 = gr.Slider(0.0, 1.0, 0.05, 0.05, label="Anger")
                emotion7 = gr.Slider(0.0, 1.0, 0.1, 0.05, label="Other")
                emotion8 = gr.Slider(0.0, 1.0, 0.2, 0.05, label="Neutral")

        with gr.Column():
            generate_button = gr.Button("Generate Audio")
            output_audio = gr.Audio(label="Generated Audio", type="numpy", autoplay=True)

        model_choice.change(
            fn=update_ui,
            inputs=[model_choice],
            outputs=[
                text,
                language,
                speaker_audio,
                prefix_audio,
                emotion1,
                emotion2,
                emotion3,
                emotion4,
                emotion5,
                emotion6,
                emotion7,
                emotion8,
                vq_single_slider,
                fmax_slider,
                pitch_std_slider,
                speaking_rate_slider,
                dnsmos_slider,
                speaker_noised_checkbox,
                unconditional_keys,
            ],
        )

        # On page load, trigger the same UI refresh
        demo.load(
            fn=update_ui,
            inputs=[model_choice],
            outputs=[
                text,
                language,
                speaker_audio,
                prefix_audio,
                emotion1,
                emotion2,
                emotion3,
                emotion4,
                emotion5,
                emotion6,
                emotion7,
                emotion8,
                vq_single_slider,
                fmax_slider,
                pitch_std_slider,
                speaking_rate_slider,
                dnsmos_slider,
                speaker_noised_checkbox,
                unconditional_keys,
            ],
        )

        # Generate audio on button click
        generate_button.click(
            fn=generate_audio,
            inputs=[
                model_choice,
                text,
                language,
                speaker_audio,
                prefix_audio,
                emotion1,
                emotion2,
                emotion3,
                emotion4,
                emotion5,
                emotion6,
                emotion7,
                emotion8,
                vq_single_slider,
                fmax_slider,
                pitch_std_slider,
                speaking_rate_slider,
                dnsmos_slider,
                speaker_noised_checkbox,
                cfg_scale_slider,
                min_p_slider,
                seed_number,
                randomize_seed_toggle,
                unconditional_keys,
            ],
            outputs=[output_audio, seed_number],
        )

    return demo


if __name__ == "__main__":
    demo = build_interface()
    share = getenv("GRADIO_SHARE", "False").lower() in ("true", "1", "t")
    demo.launch(server_name="0.0.0.0", server_port=7860, share=share)

================
File: zonos/model.py
================
import json
from typing import Callable

import safetensors
import torch
import torch.nn as nn
from huggingface_hub import hf_hub_download
from tqdm import tqdm

from .autoencoder import DACAutoencoder
from .backbone import BACKBONES
from .codebook_pattern import apply_delay_pattern, revert_delay_pattern
from .conditioning import PrefixConditioner
from .config import InferenceParams, ZonosConfig
from .sampling import sample_from_logits
from .speaker_cloning import SpeakerEmbeddingLDA
from .utils import DEFAULT_DEVICE, find_multiple, pad_weight_

DEFAULT_BACKBONE_CLS = next(iter(BACKBONES.values()))


class Zonos(nn.Module):
    def __init__(self, config: ZonosConfig, backbone_cls=DEFAULT_BACKBONE_CLS):
        super().__init__()
        self.config = config
        dim = config.backbone.d_model
        self.eos_token_id = config.eos_token_id
        self.masked_token_id = config.masked_token_id

        self.autoencoder = DACAutoencoder()
        self.backbone = backbone_cls(config.backbone)
        self.prefix_conditioner = PrefixConditioner(config.prefix_conditioner, dim)
        self.spk_clone_model = None

        # TODO: pad to multiple of at least 8
        self.embeddings = nn.ModuleList([nn.Embedding(1026, dim) for _ in range(self.autoencoder.num_codebooks)])
        self.heads = nn.ModuleList([nn.Linear(dim, 1025, bias=False) for _ in range(self.autoencoder.num_codebooks)])

        self._cg_graph = None
        self._cg_batch_size = None
        self._cg_input_ids = None
        self._cg_logits = None
        self._cg_inference_params = None
        self._cg_scale = None

        if config.pad_vocab_to_multiple_of:
            self.register_load_state_dict_post_hook(self._pad_embeddings_and_heads)

    def _pad_embeddings_and_heads(self, *args, **kwargs):
        for w in [*self.embeddings, *self.heads]:
            pad_weight_(w, self.config.pad_vocab_to_multiple_of)

    @property
    def device(self) -> torch.device:
        return next(self.parameters()).device

    @classmethod
    def from_pretrained(
        cls, repo_id: str, revision: str | None = None, device: str = DEFAULT_DEVICE, **kwargs
    ) -> "Zonos":
        config_path = hf_hub_download(repo_id=repo_id, filename="config.json", revision=revision)
        model_path = hf_hub_download(repo_id=repo_id, filename="model.safetensors", revision=revision)
        return cls.from_local(config_path, model_path, device, **kwargs)

    @classmethod
    def from_local(
        cls, config_path: str, model_path: str, device: str = DEFAULT_DEVICE, backbone: str | None = None
    ) -> "Zonos":
        config = ZonosConfig.from_dict(json.load(open(config_path)))
        if backbone:
            backbone_cls = BACKBONES[backbone]
        else:
            is_transformer = not bool(config.backbone.ssm_cfg)
            backbone_cls = DEFAULT_BACKBONE_CLS
            # Preferentially route to pure torch backbone for increased performance and lower latency.
            if is_transformer and "torch" in BACKBONES:
                backbone_cls = BACKBONES["torch"]

        model = cls(config, backbone_cls).to(device, torch.bfloat16)
        model.autoencoder.dac.to(device)

        sd = model.state_dict()
        with safetensors.safe_open(model_path, framework="pt") as f:
            for k in f.keys():
                sd[k] = f.get_tensor(k)
        model.load_state_dict(sd)

        return model

    def make_speaker_embedding(self, wav: torch.Tensor, sr: int) -> torch.Tensor:
        """Generate a speaker embedding from an audio clip."""
        if self.spk_clone_model is None:
            self.spk_clone_model = SpeakerEmbeddingLDA()
        _, spk_embedding = self.spk_clone_model(wav.to(self.spk_clone_model.device), sr)
        return spk_embedding.unsqueeze(0).bfloat16()

    def embed_codes(self, codes: torch.Tensor) -> torch.Tensor:
        return sum(emb(codes[:, i]) for i, emb in enumerate(self.embeddings))

    def apply_heads(self, hidden_states: torch.Tensor) -> torch.Tensor:
        return torch.stack([head(hidden_states) for head in self.heads], dim=1)

    def _compute_logits(
        self, hidden_states: torch.Tensor, inference_params: InferenceParams, cfg_scale: float
    ) -> torch.Tensor:
        """
        Pass `hidden_states` into `backbone` and `multi_head`, applying
        classifier-free guidance if `cfg_scale != 1.0`.
        """
        last_hidden_states = self.backbone(hidden_states, inference_params)[:, -1, :].unsqueeze(1)
        logits = self.apply_heads(last_hidden_states).squeeze(2).float()
        if cfg_scale != 1.0:
            cond_logits, uncond_logits = logits.chunk(2)
            logits = uncond_logits + (cond_logits - uncond_logits) * cfg_scale
        logits[..., 1025:].fill_(-torch.inf)  # ensures padding is ignored
        return logits

    def _decode_one_token(
        self,
        input_ids: torch.Tensor,
        inference_params: InferenceParams,
        cfg_scale: float,
        allow_cudagraphs: bool = True,
    ) -> torch.Tensor:
        """
        Single-step decode. Prepares the hidden states, possibly replicates them
        for CFG, and then delegates to `_compute_logits`.

        Below we wrap this function with a simple CUDA Graph capturing mechanism,
        doing 3 warmup steps if needed and then capturing or replaying the graph.
        We only recapture if the batch size changes.
        """
        # TODO: support cfg_scale==1
        if cfg_scale == 1.0:
            hidden_states = self.embed_codes(input_ids)
            return self._compute_logits(hidden_states, inference_params, cfg_scale)

        bsz = input_ids.size(0)

        if not allow_cudagraphs or input_ids.device.type != "cuda":
            hidden_states_local = self.embed_codes(input_ids)
            hidden_states_local = hidden_states_local.repeat(2, 1, 1)
            return self._compute_logits(hidden_states_local, inference_params, cfg_scale)

        need_capture = (self._cg_graph is None) or (self._cg_batch_size != bsz)

        if need_capture:
            self._cg_graph = None

            self._cg_batch_size = bsz
            self._cg_inference_params = inference_params
            self._cg_scale = cfg_scale

            for _ in range(3):
                hidden_states = self.embed_codes(input_ids)
                hidden_states = hidden_states.repeat(2, 1, 1)  # because cfg != 1.0
                logits = self._compute_logits(hidden_states, inference_params, cfg_scale)

            self._cg_input_ids = input_ids.clone()
            self._cg_logits = torch.empty_like(logits)

            g = torch.cuda.CUDAGraph()

            def capture_region():
                hidden_states_local = self.embed_codes(self._cg_input_ids)
                hidden_states_local = hidden_states_local.repeat(2, 1, 1)
                self._cg_logits = self._compute_logits(hidden_states_local, self._cg_inference_params, self._cg_scale)

            with torch.cuda.graph(g):
                capture_region()

            self._cg_graph = g

        else:
            self._cg_input_ids.copy_(input_ids)

        self._cg_graph.replay()

        return self._cg_logits

    def _prefill(
        self,
        prefix_hidden_states: torch.Tensor,
        input_ids: torch.Tensor,
        inference_params: InferenceParams,
        cfg_scale: float,
    ) -> torch.Tensor:
        """
        "Prefill" mode: we already have `prefix_hidden_states`, and we want
        to append new embeddings, then compute the logits.
        """
        # Replicate input_ids if CFG is enabled
        if cfg_scale != 1.0:
            input_ids = input_ids.expand(prefix_hidden_states.shape[0], -1, -1)
        hidden_states = torch.cat([prefix_hidden_states, self.embed_codes(input_ids)], dim=1)
        return self._compute_logits(hidden_states, inference_params, cfg_scale)

    def setup_cache(self, batch_size: int, max_seqlen: int, dtype: torch.dtype = torch.bfloat16) -> InferenceParams:
        max_seqlen = find_multiple(max_seqlen, 8)
        key_value_memory_dict = self.backbone.allocate_inference_cache(batch_size, max_seqlen, dtype=dtype)
        lengths_per_sample = torch.full((batch_size,), 0, dtype=torch.int32)
        return InferenceParams(max_seqlen, batch_size, 0, 0, key_value_memory_dict, lengths_per_sample)

    def prepare_conditioning(self, cond_dict: dict, uncond_dict: dict | None = None) -> torch.Tensor:
        if uncond_dict is None:
            uncond_dict = {k: cond_dict[k] for k in self.prefix_conditioner.required_keys}
        return torch.cat(
            [
                self.prefix_conditioner(cond_dict),
                self.prefix_conditioner(uncond_dict),
            ]
        )

    def can_use_cudagraphs(self) -> bool:
        # Only the mamba-ssm backbone supports CUDA Graphs at the moment
        return self.device.type == "cuda" and "_mamba_ssm" in str(self.backbone.__class__)

    @torch.inference_mode()
    def generate(
        self,
        prefix_conditioning: torch.Tensor,  # [bsz, cond_seq_len, d_model]
        audio_prefix_codes: torch.Tensor | None = None,  # [bsz, 9, prefix_audio_seq_len]
        max_new_tokens: int = 86 * 30,
        cfg_scale: float = 2.0,
        batch_size: int = 1,
        sampling_params: dict = dict(min_p=0.1),
        progress_bar: bool = True,
        disable_torch_compile: bool = False,
        callback: Callable[[torch.Tensor, int, int], bool] | None = None,
    ):
        assert cfg_scale != 1, "TODO: add support for cfg_scale=1"
        prefix_audio_len = 0 if audio_prefix_codes is None else audio_prefix_codes.shape[2]
        device = self.device

        # Use CUDA Graphs if supported, and torch.compile otherwise.
        cg = self.can_use_cudagraphs()
        decode_one_token = self._decode_one_token
        decode_one_token = torch.compile(decode_one_token, dynamic=True, disable=cg or disable_torch_compile)

        unknown_token = -1
        audio_seq_len = prefix_audio_len + max_new_tokens
        seq_len = prefix_conditioning.shape[1] + audio_seq_len + 9

        with torch.device(device):
            inference_params = self.setup_cache(batch_size=batch_size * 2, max_seqlen=seq_len)
            codes = torch.full((batch_size, 9, audio_seq_len), unknown_token)

        if audio_prefix_codes is not None:
            codes[..., :prefix_audio_len] = audio_prefix_codes

        delayed_codes = apply_delay_pattern(codes, self.masked_token_id)

        delayed_prefix_audio_codes = delayed_codes[..., : prefix_audio_len + 1]

        logits = self._prefill(prefix_conditioning, delayed_prefix_audio_codes, inference_params, cfg_scale)
        next_token = sample_from_logits(logits, **sampling_params)

        offset = delayed_prefix_audio_codes.shape[2]
        frame = delayed_codes[..., offset : offset + 1]
        frame.masked_scatter_(frame == unknown_token, next_token)

        prefix_length = prefix_conditioning.shape[1] + prefix_audio_len + 1
        inference_params.seqlen_offset += prefix_length
        inference_params.lengths_per_sample[:] += prefix_length

        logit_bias = torch.zeros_like(logits)
        logit_bias[:, 1:, self.eos_token_id] = -torch.inf  # only allow codebook 0 to predict EOS

        stopping = torch.zeros(batch_size, dtype=torch.bool, device=device)
        max_steps = delayed_codes.shape[2] - offset
        remaining_steps = torch.full((batch_size,), max_steps, device=device)
        progress = tqdm(total=max_steps, desc="Generating", disable=not progress_bar)
        cfg_scale = torch.tensor(cfg_scale)

        step = 0
        while torch.max(remaining_steps) > 0:
            offset += 1
            input_ids = delayed_codes[..., offset - 1 : offset]
            logits = decode_one_token(input_ids, inference_params, cfg_scale, allow_cudagraphs=cg)
            logits += logit_bias

            next_token = sample_from_logits(logits, generated_tokens=delayed_codes[..., :offset], **sampling_params)
            eos_in_cb0 = next_token[:, 0] == self.eos_token_id

            remaining_steps[eos_in_cb0[:, 0]] = torch.minimum(remaining_steps[eos_in_cb0[:, 0]], torch.tensor(9))
            stopping |= eos_in_cb0[:, 0]

            eos_codebook_idx = 9 - remaining_steps
            eos_codebook_idx = torch.clamp(eos_codebook_idx, max=9 - 1)
            for i in range(next_token.shape[0]):
                if stopping[i]:
                    idx = eos_codebook_idx[i].item()
                    next_token[i, :idx] = self.masked_token_id
                    next_token[i, idx] = self.eos_token_id

            frame = delayed_codes[..., offset : offset + 1]
            frame.masked_scatter_(frame == unknown_token, next_token)
            inference_params.seqlen_offset += 1
            inference_params.lengths_per_sample[:] += 1

            remaining_steps -= 1

            progress.update()
            step += 1

            if callback is not None and not callback(frame, step, max_steps):
                break

        out_codes = revert_delay_pattern(delayed_codes)
        out_codes.masked_fill_(out_codes >= 1024, 0)
        out_codes = out_codes[..., : offset - 9]

        self._cg_graph = None  # reset cuda graph to avoid cache changes

        return out_codes

================
File: zonos/sample.py
================
import torch
import torchaudio
from .model import Zonos
from .conditioning import make_cond_dict
from .utils import DEFAULT_DEVICE as device

# model = Zonos.from_pretrained("Zyphra/Zonos-v0.1-hybrid", device=device)
model = Zonos.from_pretrained("Zyphra/Zonos-v0.1-transformer", device=device)

wav, sampling_rate = torchaudio.load("assets/exampleaudio.mp3")
speaker = model.make_speaker_embedding(wav, sampling_rate)

torch.manual_seed(421)

cond_dict = make_cond_dict(text="Hello, world!", speaker=speaker, language="en-us")
conditioning = model.prepare_conditioning(cond_dict)

codes = model.generate(conditioning)

wavs = model.autoencoder.decode(codes).cpu()
torchaudio.save("sample.wav", wavs[0], model.autoencoder.sampling_rate)

================
File: zonos/sampling.py
================
import torch


def multinomial(input: torch.Tensor, num_samples: int, replacement=False, *, generator=None):
    """torch.multinomial with arbitrary number of dimensions, and number of candidates on the last dimension.

    Args:
        input (torch.Tensor): The input tensor containing probabilities.
        num_samples (int): Number of samples to draw.
        replacement (bool): Whether to draw with replacement or not.
    Keywords args:
        generator (torch.Generator): A pseudorandom number generator for sampling.
    Returns:
        torch.Tensor: Last dimension contains num_samples indices
            sampled from the multinomial probability distribution
            located in the last dimension of tensor input.
    """

    if num_samples == 1:
        q = torch.empty_like(input).exponential_(1, generator=generator)
        return torch.argmax(input / q, dim=-1, keepdim=True).to(torch.int64)

    input_ = input.reshape(-1, input.shape[-1])
    output_ = torch.multinomial(input_, num_samples=num_samples, replacement=replacement, generator=generator)
    output = output_.reshape(*list(input.shape[:-1]), -1)
    return output


def apply_top_k(
    probs: torch.Tensor,
    k: int,
) -> torch.Tensor:
    """Sample next token from top K values along the last dimension of the input probs tensor.

    Args:
        probs (torch.Tensor): Input probabilities with token candidates on the last dimension.
        k (int): The k in â€œtop-kâ€.
    Returns:
        torch.Tensor: Sampled tokens.
    """
    v, _ = torch.topk(probs, min(k, probs.size(-1)))
    pivot = v.select(-1, -1).unsqueeze(-1)
    probs = torch.where(probs < pivot, 0.0, probs)
    probs.div_(probs.sum(dim=-1, keepdim=True))
    return probs


def apply_top_p(probs: torch.Tensor, p: float) -> torch.Tensor:
    """Sample next token from top P probabilities along the last dimension of the input probs tensor.

    Args:
        probs (torch.Tensor): Input probabilities with token candidates on the last dimension.
        p (int): The p in â€œtop-pâ€.
    Returns:
        torch.Tensor: Sampled tokens.
    """
    probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)
    probs_sum = torch.cumsum(probs_sort, dim=-1)
    mask = probs_sum - probs_sort > p
    probs_sort *= (~mask).float()
    probs = probs.scatter(-1, probs_idx, probs_sort)
    probs.div_(probs.sum(dim=-1, keepdim=True))
    return probs


def apply_min_p(probs: torch.Tensor, min_p: float) -> torch.Tensor:
    """Sample next token using min-p sampling.

    Args:
        scores (torch.FloatTensor): Input logits with token candidates on the last dimension.
        min_p (float): Minimum token probability, scaled by the probability of the most likely token.
                       Must be between 0 and 1. Typical values are in the 0.01-0.2 range.
    Returns:
        torch.Tensor: Sampled tokens.
    """
    top_probs, _ = probs.max(dim=-1, keepdim=True)
    tokens_to_remove = probs < (min_p * top_probs)
    probs = probs.masked_fill(tokens_to_remove, 0.0)
    probs.div_(probs.sum(dim=-1, keepdim=True))
    return probs


def modify_logit_for_repetition_penalty(
    logits: torch.Tensor,
    generated_tokens: torch.Tensor,
    repetition_penalty: float,
    repetition_penalty_window: int,
):
    """See https://arxiv.org/abs/1909.05858
    Apply repetition penalty over a sliding window of the last `repetition_penalty_window` tokens.
    logits: (batch_size, n_codebooks, vocab_size)
    generated_tokens: (batch_size, n_codebooks, seq_len)
    """
    generated_tokens = generated_tokens[..., -repetition_penalty_window:]
    generated_tokens = generated_tokens.clamp_max(logits.shape[-1] - 1).to(torch.int64)
    rp = torch.full_like(logits, repetition_penalty)
    factors = torch.ones_like(logits).scatter_reduce(2, generated_tokens, rp, reduce="prod")
    return torch.where(logits <= 0, logits * factors, logits / factors)


def sample_from_logits(
    logits: torch.Tensor,
    temperature: float = 1.0,
    top_p: float = 0.0,
    top_k: int = 0,
    min_p: float = 0.0,
    generated_tokens: torch.Tensor | None = None,
    repetition_penalty: float = 3.0,
    repetition_penalty_window: int = 2,
) -> torch.Tensor:
    """Sample next token from logits using temperature, top-p, top-k, or min-p sampling.

    Args:
        logits (torch.Tensor): Input logits with token candidates on the last dimension.
        temperature (float): Sampling temperature. Lower temperature results in more deterministic samples.
        top_p (float): The p in â€œtop-pâ€.
        top_k (int): The k in â€œtop-kâ€.
        min_p (float): Minimum token probability, scaled by the probability of the most likely token.
                       Must be between 0 and 1. Typical values are in the 0.01-0.2 range.

    Returns:
        torch.Tensor: Sampled tokens.
    """
    if repetition_penalty != 1.0 and generated_tokens is not None:
        logits = modify_logit_for_repetition_penalty(logits, generated_tokens, repetition_penalty, repetition_penalty_window)

    if temperature > 0:
        probs = torch.softmax(logits / temperature, dim=-1)

        if top_p > 0:
            probs = apply_top_p(probs, top_p)
        if top_k > 0:
            probs = apply_top_k(probs, top_k)
        if min_p > 0:
            probs = apply_min_p(probs, min_p)

        next_token = multinomial(probs, num_samples=1)
    else:
        next_token = torch.argmax(logits, dim=-1, keepdim=True)

    return next_token  # [batch_size, num_codebooks, 1]

================
File: zonos/speaker_cloning.py
================
import math
from functools import cache

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchaudio
from huggingface_hub import hf_hub_download

from .utils import DEFAULT_DEVICE


class logFbankCal(nn.Module):
    def __init__(
        self,
        sample_rate: int = 16_000,
        n_fft: int = 512,
        win_length: float = 0.025,
        hop_length: float = 0.01,
        n_mels: int = 80,
    ):
        super().__init__()
        self.fbankCal = torchaudio.transforms.MelSpectrogram(
            sample_rate=sample_rate,
            n_fft=n_fft,
            win_length=int(win_length * sample_rate),
            hop_length=int(hop_length * sample_rate),
            n_mels=n_mels,
        )

    def forward(self, x):
        out = self.fbankCal(x)
        out = torch.log(out + 1e-6)
        out = out - out.mean(axis=2).unsqueeze(dim=2)
        return out


class ASP(nn.Module):
    # Attentive statistics pooling
    def __init__(self, in_planes, acoustic_dim):
        super(ASP, self).__init__()
        outmap_size = int(acoustic_dim / 8)
        self.out_dim = in_planes * 8 * outmap_size * 2

        self.attention = nn.Sequential(
            nn.Conv1d(in_planes * 8 * outmap_size, 128, kernel_size=1),
            nn.ReLU(),
            nn.BatchNorm1d(128),
            nn.Conv1d(128, in_planes * 8 * outmap_size, kernel_size=1),
            nn.Softmax(dim=2),
        )

    def forward(self, x):
        x = x.reshape(x.size()[0], -1, x.size()[-1])
        w = self.attention(x)
        mu = torch.sum(x * w, dim=2)
        sg = torch.sqrt((torch.sum((x**2) * w, dim=2) - mu**2).clamp(min=1e-5))
        x = torch.cat((mu, sg), 1)

        x = x.view(x.size()[0], -1)
        return x


class SimAMBasicBlock(nn.Module):
    expansion = 1

    def __init__(self, ConvLayer, NormLayer, in_planes, planes, stride=1, block_id=1):
        super(SimAMBasicBlock, self).__init__()
        self.conv1 = ConvLayer(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn1 = NormLayer(planes)
        self.conv2 = ConvLayer(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn2 = NormLayer(planes)
        self.relu = nn.ReLU(inplace=True)
        self.sigmoid = nn.Sigmoid()

        self.downsample = nn.Sequential()
        if stride != 1 or in_planes != self.expansion * planes:
            self.downsample = nn.Sequential(
                ConvLayer(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),
                NormLayer(self.expansion * planes),
            )

    def forward(self, x):
        out = self.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out = self.SimAM(out)
        out += self.downsample(x)
        out = self.relu(out)
        return out

    def SimAM(self, X, lambda_p=1e-4):
        n = X.shape[2] * X.shape[3] - 1
        d = (X - X.mean(dim=[2, 3], keepdim=True)).pow(2)
        v = d.sum(dim=[2, 3], keepdim=True) / n
        E_inv = d / (4 * (v + lambda_p)) + 0.5
        return X * self.sigmoid(E_inv)


class BasicBlock(nn.Module):
    expansion = 1

    def __init__(self, ConvLayer, NormLayer, in_planes, planes, stride=1, block_id=1):
        super(BasicBlock, self).__init__()
        self.conv1 = ConvLayer(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn1 = NormLayer(planes)
        self.conv2 = ConvLayer(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn2 = NormLayer(planes)
        self.relu = nn.ReLU(inplace=True)

        self.downsample = nn.Sequential()
        if stride != 1 or in_planes != self.expansion * planes:
            self.downsample = nn.Sequential(
                ConvLayer(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),
                NormLayer(self.expansion * planes),
            )

    def forward(self, x):
        out = self.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += self.downsample(x)
        out = self.relu(out)
        return out


class Bottleneck(nn.Module):
    expansion = 4

    def __init__(self, ConvLayer, NormLayer, in_planes, planes, stride=1, block_id=1):
        super(Bottleneck, self).__init__()
        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)
        self.conv3 = nn.Conv2d(planes, self.expansion * planes, kernel_size=1, bias=False)
        self.bn3 = nn.BatchNorm2d(self.expansion * planes)

        self.shortcut = nn.Sequential()
        if stride != 1 or in_planes != self.expansion * planes:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(self.expansion * planes),
            )

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = F.relu(self.bn2(self.conv2(out)))
        out = self.bn3(self.conv3(out))
        out += self.shortcut(x)
        out = F.relu(out)
        return out


class ResNet(nn.Module):
    def __init__(self, in_planes, block, num_blocks, in_ch=1, feat_dim="2d", **kwargs):
        super(ResNet, self).__init__()
        if feat_dim == "1d":
            self.NormLayer = nn.BatchNorm1d
            self.ConvLayer = nn.Conv1d
        elif feat_dim == "2d":
            self.NormLayer = nn.BatchNorm2d
            self.ConvLayer = nn.Conv2d
        elif feat_dim == "3d":
            self.NormLayer = nn.BatchNorm3d
            self.ConvLayer = nn.Conv3d
        else:
            print("error")

        self.in_planes = in_planes

        self.conv1 = self.ConvLayer(in_ch, in_planes, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn1 = self.NormLayer(in_planes)
        self.relu = nn.ReLU(inplace=True)
        self.layer1 = self._make_layer(block, in_planes, num_blocks[0], stride=1, block_id=1)
        self.layer2 = self._make_layer(block, in_planes * 2, num_blocks[1], stride=2, block_id=2)
        self.layer3 = self._make_layer(block, in_planes * 4, num_blocks[2], stride=2, block_id=3)
        self.layer4 = self._make_layer(block, in_planes * 8, num_blocks[3], stride=2, block_id=4)

    def _make_layer(self, block, planes, num_blocks, stride, block_id=1):
        strides = [stride] + [1] * (num_blocks - 1)
        layers = []
        for stride in strides:
            layers.append(block(self.ConvLayer, self.NormLayer, self.in_planes, planes, stride, block_id))
            self.in_planes = planes * block.expansion
        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.relu(self.bn1(self.conv1(x)))
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        return x


def ResNet293(in_planes: int, **kwargs):
    return ResNet(in_planes, SimAMBasicBlock, [10, 20, 64, 3], **kwargs)


class ResNet293_based(nn.Module):
    def __init__(
        self,
        in_planes: int = 64,
        embd_dim: int = 256,
        acoustic_dim: int = 80,
        featCal=None,
        dropout: float = 0,
        **kwargs,
    ):
        super(ResNet293_based, self).__init__()
        self.featCal = featCal
        self.front = ResNet293(in_planes)
        block_expansion = SimAMBasicBlock.expansion
        self.pooling = ASP(in_planes * block_expansion, acoustic_dim)
        self.bottleneck = nn.Linear(self.pooling.out_dim, embd_dim)
        self.drop = nn.Dropout(dropout) if dropout else None

    def forward(self, x):
        x = self.featCal(x)
        x = self.front(x.unsqueeze(dim=1))
        x = self.pooling(x)
        if self.drop:
            x = self.drop(x)
        x = self.bottleneck(x)
        return x


class SEModule(nn.Module):
    def __init__(self, channels, bottleneck=128):
        super(SEModule, self).__init__()
        self.se = nn.Sequential(
            nn.AdaptiveAvgPool1d(1),
            nn.Conv1d(channels, bottleneck, kernel_size=1, padding=0),
            nn.ReLU(),
            # nn.BatchNorm1d(bottleneck), # Removed
            nn.Conv1d(bottleneck, channels, kernel_size=1, padding=0),
            nn.Sigmoid(),
        )

    def forward(self, input):
        x = self.se(input)
        return input * x


class Bottle2neck(nn.Module):
    def __init__(self, inplanes, planes, kernel_size=None, dilation=None, scale=8):
        super(Bottle2neck, self).__init__()
        width = int(math.floor(planes / scale))
        self.conv1 = nn.Conv1d(inplanes, width * scale, kernel_size=1)
        self.bn1 = nn.BatchNorm1d(width * scale)
        self.nums = scale - 1
        convs = []
        bns = []
        num_pad = math.floor(kernel_size / 2) * dilation
        for i in range(self.nums):
            convs.append(nn.Conv1d(width, width, kernel_size=kernel_size, dilation=dilation, padding=num_pad))
            bns.append(nn.BatchNorm1d(width))
        self.convs = nn.ModuleList(convs)
        self.bns = nn.ModuleList(bns)
        self.conv3 = nn.Conv1d(width * scale, planes, kernel_size=1)
        self.bn3 = nn.BatchNorm1d(planes)
        self.relu = nn.ReLU()
        self.width = width
        self.se = SEModule(planes)

    def forward(self, x):
        residual = x
        out = self.conv1(x)
        out = self.relu(out)
        out = self.bn1(out)

        spx = torch.split(out, self.width, 1)
        for i in range(self.nums):
            if i == 0:
                sp = spx[i]
            else:
                sp = sp + spx[i]
            sp = self.convs[i](sp)
            sp = self.relu(sp)
            sp = self.bns[i](sp)
            if i == 0:
                out = sp
            else:
                out = torch.cat((out, sp), 1)
        out = torch.cat((out, spx[self.nums]), 1)

        out = self.conv3(out)
        out = self.relu(out)
        out = self.bn3(out)

        out = self.se(out)
        out += residual
        return out


class ECAPA_TDNN(nn.Module):
    def __init__(self, C, featCal):
        super(ECAPA_TDNN, self).__init__()
        self.featCal = featCal
        self.conv1 = nn.Conv1d(80, C, kernel_size=5, stride=1, padding=2)
        self.relu = nn.ReLU()
        self.bn1 = nn.BatchNorm1d(C)
        self.layer1 = Bottle2neck(C, C, kernel_size=3, dilation=2, scale=8)
        self.layer2 = Bottle2neck(C, C, kernel_size=3, dilation=3, scale=8)
        self.layer3 = Bottle2neck(C, C, kernel_size=3, dilation=4, scale=8)
        # I fixed the shape of the output from MFA layer, that is close to the setting from ECAPA paper.
        self.layer4 = nn.Conv1d(3 * C, 1536, kernel_size=1)
        self.attention = nn.Sequential(
            nn.Conv1d(4608, 256, kernel_size=1),
            nn.ReLU(),
            nn.BatchNorm1d(256),
            nn.Tanh(),  # Added
            nn.Conv1d(256, 1536, kernel_size=1),
            nn.Softmax(dim=2),
        )
        self.bn5 = nn.BatchNorm1d(3072)
        self.fc6 = nn.Linear(3072, 192)
        self.bn6 = nn.BatchNorm1d(192)

    def forward(self, x):
        x = self.featCal(x)
        x = self.conv1(x)
        x = self.relu(x)
        x = self.bn1(x)

        x1 = self.layer1(x)
        x2 = self.layer2(x + x1)
        x3 = self.layer3(x + x1 + x2)

        x = self.layer4(torch.cat((x1, x2, x3), dim=1))
        x = self.relu(x)

        t = x.size()[-1]

        global_x = torch.cat(
            (
                x,
                torch.mean(x, dim=2, keepdim=True).repeat(1, 1, t),
                torch.sqrt(torch.var(x, dim=2, keepdim=True).clamp(min=1e-4)).repeat(1, 1, t),
            ),
            dim=1,
        )

        w = self.attention(global_x)

        mu = torch.sum(x * w, dim=2)
        sg = torch.sqrt((torch.sum((x**2) * w, dim=2) - mu**2).clamp(min=1e-4))

        x = torch.cat((mu, sg), 1)
        x = self.bn5(x)
        x = self.fc6(x)
        x = self.bn6(x)

        return x


class SpeakerEmbedding(nn.Module):
    def __init__(self, ckpt_path: str = "ResNet293_SimAM_ASP_base.pt", device: str = DEFAULT_DEVICE):
        super().__init__()
        self.device = device
        with torch.device(device):
            self.model = ResNet293_based()
            state_dict = torch.load(ckpt_path, weights_only=True, mmap=True, map_location="cpu")
            self.model.load_state_dict(state_dict)
            self.model.featCal = logFbankCal()

        self.requires_grad_(False).eval()

    @property
    def dtype(self):
        return next(self.parameters()).dtype

    @cache
    def _get_resampler(self, orig_sample_rate: int):
        return torchaudio.transforms.Resample(orig_sample_rate, 16_000).to(self.device)

    def prepare_input(self, wav: torch.Tensor, sample_rate: int) -> torch.Tensor:
        assert wav.ndim < 3
        if wav.ndim == 2:
            wav = wav.mean(0, keepdim=True)
        wav = self._get_resampler(sample_rate)(wav)
        return wav

    def forward(self, wav: torch.Tensor, sample_rate: int):
        wav = self.prepare_input(wav, sample_rate).to(self.device, self.dtype)
        return self.model(wav).to(wav.device)


class SpeakerEmbeddingLDA(nn.Module):
    def __init__(self, device: str = DEFAULT_DEVICE):
        super().__init__()
        spk_model_path = hf_hub_download(
            repo_id="Zyphra/Zonos-v0.1-speaker-embedding",
            filename="ResNet293_SimAM_ASP_base.pt",
        )
        lda_spk_model_path = hf_hub_download(
            repo_id="Zyphra/Zonos-v0.1-speaker-embedding",
            filename="ResNet293_SimAM_ASP_base_LDA-128.pt",
        )

        self.device = device
        with torch.device(device):
            self.model = SpeakerEmbedding(spk_model_path, device)
            lda_sd = torch.load(lda_spk_model_path, weights_only=True)
            out_features, in_features = lda_sd["weight"].shape
            self.lda = nn.Linear(in_features, out_features, bias=True, dtype=torch.float32)
            self.lda.load_state_dict(lda_sd)

        self.requires_grad_(False).eval()

    def forward(self, wav: torch.Tensor, sample_rate: int):
        emb = self.model(wav, sample_rate).to(torch.float32)
        return emb, self.lda(emb)

================
File: zonos/utils.py
================
import torch
import torch.nn as nn
import torch.nn.functional as F


def find_multiple(n: int, k: int) -> int:
    if k == 0 or n % k == 0:
        return n
    return n + k - (n % k)


def pad_weight_(w: nn.Embedding | nn.Linear, multiple: int):
    """Pad the weight of an embedding or linear layer to a multiple of `multiple`."""
    if isinstance(w, nn.Embedding):
        # Pad input dim
        if w.weight.shape[1] % multiple == 0:
            return
        w.weight.data = F.pad(w.weight.data, (0, 0, 0, w.weight.shape[1] % multiple))
        w.num_embeddings, w.embedding_dim = w.weight.shape
    elif isinstance(w, nn.Linear):
        # Pad output dim
        if w.weight.shape[0] % multiple == 0:
            return
        w.weight.data = F.pad(w.weight.data, (0, 0, 0, w.weight.shape[0] % multiple))
        w.out_features, w.in_features = w.weight.shape
    else:
        raise ValueError(f"Unsupported weight type: {type(w)}")


def get_device() -> torch.device:
    if torch.cuda.is_available():
        return torch.device(torch.cuda.current_device())
    # MPS breaks for whatever reason. Uncomment when it's working.
    # if torch.mps.is_available():
    #     return torch.device("mps")
    return torch.device("cpu")


DEFAULT_DEVICE = get_device()
